{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_PyTorch_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMIgqUv8IkQWgRXFwpkk/E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiyazovIlia/PyTorch-1/blob/lesson-8/HW_PyTorch_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wd2DMzlyhrW"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_length = 1024\n",
        "train_data = torch.zeros((train_data_length, 2))\n",
        "train_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\n",
        "train_data[:, 1] = (torch.sin(train_data[:, 0])/train_data[:, 0]) - (train_data[:, 0]/10)\n",
        "train_labels = torch.zeros(train_data_length)\n",
        "train_set = [(train_data[i], train_labels[i]) for i in range(train_data_length)]"
      ],
      "metadata": {
        "id": "2X0Y3d8KymWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXDbno5pzhVE",
        "outputId": "fdc307db-3603-4ccc-b1a5-522b4e396867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([ 3.7450, -0.5260]), tensor(0.)),\n",
              " (tensor([0.3463, 0.9455]), tensor(0.)),\n",
              " (tensor([ 5.6665, -0.6687]), tensor(0.)),\n",
              " (tensor([0.4957, 0.9100]), tensor(0.)),\n",
              " (tensor([2.2053, 0.1447]), tensor(0.)),\n",
              " (tensor([ 5.9295, -0.6514]), tensor(0.)),\n",
              " (tensor([ 5.3492, -0.6852]), tensor(0.)),\n",
              " (tensor([0.9680, 0.7542]), tensor(0.)),\n",
              " (tensor([ 5.8133, -0.6592]), tensor(0.)),\n",
              " (tensor([2.1580, 0.1700]), tensor(0.)),\n",
              " (tensor([ 6.2771, -0.6287]), tensor(0.)),\n",
              " (tensor([ 5.3748, -0.6842]), tensor(0.)),\n",
              " (tensor([ 2.9895, -0.2483]), tensor(0.)),\n",
              " (tensor([2.0337, 0.2366]), tensor(0.)),\n",
              " (tensor([ 5.2584, -0.6884]), tensor(0.)),\n",
              " (tensor([ 5.4631, -0.6802]), tensor(0.)),\n",
              " (tensor([ 3.1145, -0.3028]), tensor(0.)),\n",
              " (tensor([ 3.0333, -0.2677]), tensor(0.)),\n",
              " (tensor([1.0687, 0.7134]), tensor(0.)),\n",
              " (tensor([ 2.7712, -0.1465]), tensor(0.)),\n",
              " (tensor([0.1439, 0.9822]), tensor(0.)),\n",
              " (tensor([2.3943, 0.0445]), tensor(0.)),\n",
              " (tensor([ 4.0241, -0.5943]), tensor(0.)),\n",
              " (tensor([ 5.1249, -0.6912]), tensor(0.)),\n",
              " (tensor([ 5.9259, -0.6516]), tensor(0.)),\n",
              " (tensor([ 3.1388, -0.3130]), tensor(0.)),\n",
              " (tensor([ 5.5733, -0.6743]), tensor(0.)),\n",
              " (tensor([ 3.1653, -0.3240]), tensor(0.)),\n",
              " (tensor([ 5.0963, -0.6916]), tensor(0.)),\n",
              " (tensor([0.0974, 0.9887]), tensor(0.)),\n",
              " (tensor([ 6.2151, -0.6325]), tensor(0.)),\n",
              " (tensor([1.7680, 0.3778]), tensor(0.)),\n",
              " (tensor([ 4.7882, -0.6871]), tensor(0.)),\n",
              " (tensor([ 5.0036, -0.6918]), tensor(0.)),\n",
              " (tensor([ 2.6898, -0.1067]), tensor(0.)),\n",
              " (tensor([1.4971, 0.5164]), tensor(0.)),\n",
              " (tensor([ 3.6357, -0.4940]), tensor(0.)),\n",
              " (tensor([ 5.6838, -0.6676]), tensor(0.)),\n",
              " (tensor([ 5.0652, -0.6918]), tensor(0.)),\n",
              " (tensor([ 6.0467, -0.6434]), tensor(0.)),\n",
              " (tensor([ 5.6672, -0.6687]), tensor(0.)),\n",
              " (tensor([0.3142, 0.9522]), tensor(0.)),\n",
              " (tensor([ 4.3843, -0.6544]), tensor(0.)),\n",
              " (tensor([2.3582, 0.0634]), tensor(0.)),\n",
              " (tensor([0.3331, 0.9483]), tensor(0.)),\n",
              " (tensor([ 2.7790, -0.1503]), tensor(0.)),\n",
              " (tensor([1.3661, 0.5801]), tensor(0.)),\n",
              " (tensor([ 4.5165, -0.6688]), tensor(0.)),\n",
              " (tensor([ 3.2379, -0.3535]), tensor(0.)),\n",
              " (tensor([0.9058, 0.7782]), tensor(0.)),\n",
              " (tensor([ 3.0805, -0.2882]), tensor(0.)),\n",
              " (tensor([ 2.5620, -0.0425]), tensor(0.)),\n",
              " (tensor([ 4.8194, -0.6882]), tensor(0.)),\n",
              " (tensor([ 5.8287, -0.6582]), tensor(0.)),\n",
              " (tensor([ 2.9860, -0.2467]), tensor(0.)),\n",
              " (tensor([0.0029, 0.9997]), tensor(0.)),\n",
              " (tensor([ 4.8035, -0.6877]), tensor(0.)),\n",
              " (tensor([ 4.0998, -0.6095]), tensor(0.)),\n",
              " (tensor([1.0472, 0.7223]), tensor(0.)),\n",
              " (tensor([ 3.6863, -0.5092]), tensor(0.)),\n",
              " (tensor([0.5989, 0.8814]), tensor(0.)),\n",
              " (tensor([ 6.1398, -0.6373]), tensor(0.)),\n",
              " (tensor([ 3.5962, -0.4817]), tensor(0.)),\n",
              " (tensor([ 5.4877, -0.6789]), tensor(0.)),\n",
              " (tensor([1.0919, 0.7036]), tensor(0.)),\n",
              " (tensor([ 5.0453, -0.6919]), tensor(0.)),\n",
              " (tensor([ 4.4880, -0.6660]), tensor(0.)),\n",
              " (tensor([ 3.3348, -0.3911]), tensor(0.)),\n",
              " (tensor([ 3.3890, -0.4112]), tensor(0.)),\n",
              " (tensor([1.7323, 0.3965]), tensor(0.)),\n",
              " (tensor([ 3.8723, -0.5596]), tensor(0.)),\n",
              " (tensor([1.5390, 0.4956]), tensor(0.)),\n",
              " (tensor([0.1141, 0.9864]), tensor(0.)),\n",
              " (tensor([0.0198, 0.9979]), tensor(0.)),\n",
              " (tensor([ 4.0551, -0.6007]), tensor(0.)),\n",
              " (tensor([ 5.2644, -0.6882]), tensor(0.)),\n",
              " (tensor([ 4.0440, -0.5985]), tensor(0.)),\n",
              " (tensor([ 4.4852, -0.6657]), tensor(0.)),\n",
              " (tensor([ 2.8612, -0.1894]), tensor(0.)),\n",
              " (tensor([ 4.7820, -0.6868]), tensor(0.)),\n",
              " (tensor([ 4.2932, -0.6421]), tensor(0.)),\n",
              " (tensor([ 4.2791, -0.6400]), tensor(0.)),\n",
              " (tensor([2.3933, 0.0450]), tensor(0.)),\n",
              " (tensor([ 2.9897, -0.2484]), tensor(0.)),\n",
              " (tensor([0.6903, 0.8534]), tensor(0.)),\n",
              " (tensor([ 4.5871, -0.6750]), tensor(0.)),\n",
              " (tensor([2.4184, 0.0318]), tensor(0.)),\n",
              " (tensor([ 2.8891, -0.2024]), tensor(0.)),\n",
              " (tensor([ 2.5774, -0.0503]), tensor(0.)),\n",
              " (tensor([ 4.1545, -0.6197]), tensor(0.)),\n",
              " (tensor([1.4755, 0.5271]), tensor(0.)),\n",
              " (tensor([ 5.2309, -0.6891]), tensor(0.)),\n",
              " (tensor([1.3574, 0.5842]), tensor(0.)),\n",
              " (tensor([ 5.0493, -0.6918]), tensor(0.)),\n",
              " (tensor([ 3.2644, -0.3640]), tensor(0.)),\n",
              " (tensor([ 4.2472, -0.6351]), tensor(0.)),\n",
              " (tensor([1.8848, 0.3161]), tensor(0.)),\n",
              " (tensor([2.4525, 0.0140]), tensor(0.)),\n",
              " (tensor([ 4.2169, -0.6303]), tensor(0.)),\n",
              " (tensor([0.0762, 0.9914]), tensor(0.)),\n",
              " (tensor([0.8036, 0.8154]), tensor(0.)),\n",
              " (tensor([0.3778, 0.9386]), tensor(0.)),\n",
              " (tensor([1.5954, 0.4671]), tensor(0.)),\n",
              " (tensor([ 2.7537, -0.1380]), tensor(0.)),\n",
              " (tensor([2.3067, 0.0907]), tensor(0.)),\n",
              " (tensor([0.9469, 0.7624]), tensor(0.)),\n",
              " (tensor([1.5620, 0.4840]), tensor(0.)),\n",
              " (tensor([ 2.6953, -0.1094]), tensor(0.)),\n",
              " (tensor([0.5495, 0.8955]), tensor(0.)),\n",
              " (tensor([ 5.7942, -0.6605]), tensor(0.)),\n",
              " (tensor([0.7435, 0.8360]), tensor(0.)),\n",
              " (tensor([0.0473, 0.9949]), tensor(0.)),\n",
              " (tensor([ 6.0284, -0.6446]), tensor(0.)),\n",
              " (tensor([1.8810, 0.3182]), tensor(0.)),\n",
              " (tensor([ 3.5118, -0.4542]), tensor(0.)),\n",
              " (tensor([1.3951, 0.5662]), tensor(0.)),\n",
              " (tensor([ 5.3613, -0.6847]), tensor(0.)),\n",
              " (tensor([ 3.4104, -0.4189]), tensor(0.)),\n",
              " (tensor([ 3.7826, -0.5364]), tensor(0.)),\n",
              " (tensor([ 3.1867, -0.3328]), tensor(0.)),\n",
              " (tensor([2.3946, 0.0443]), tensor(0.)),\n",
              " (tensor([2.3047, 0.0917]), tensor(0.)),\n",
              " (tensor([ 5.4491, -0.6808]), tensor(0.)),\n",
              " (tensor([ 5.8765, -0.6550]), tensor(0.)),\n",
              " (tensor([2.4229, 0.0295]), tensor(0.)),\n",
              " (tensor([ 4.2488, -0.6354]), tensor(0.)),\n",
              " (tensor([ 4.9548, -0.6914]), tensor(0.)),\n",
              " (tensor([ 3.5471, -0.4659]), tensor(0.)),\n",
              " (tensor([1.2701, 0.6250]), tensor(0.)),\n",
              " (tensor([ 5.6253, -0.6712]), tensor(0.)),\n",
              " (tensor([0.4600, 0.9191]), tensor(0.)),\n",
              " (tensor([ 2.5765, -0.0498]), tensor(0.)),\n",
              " (tensor([0.5677, 0.8904]), tensor(0.)),\n",
              " (tensor([ 3.1042, -0.2984]), tensor(0.)),\n",
              " (tensor([ 5.9724, -0.6484]), tensor(0.)),\n",
              " (tensor([1.9166, 0.2992]), tensor(0.)),\n",
              " (tensor([ 2.5168, -0.0193]), tensor(0.)),\n",
              " (tensor([2.2558, 0.1177]), tensor(0.)),\n",
              " (tensor([0.4751, 0.9153]), tensor(0.)),\n",
              " (tensor([ 5.9377, -0.6508]), tensor(0.)),\n",
              " (tensor([1.7580, 0.3831]), tensor(0.)),\n",
              " (tensor([0.9038, 0.7789]), tensor(0.)),\n",
              " (tensor([ 2.5214, -0.0217]), tensor(0.)),\n",
              " (tensor([ 5.7389, -0.6641]), tensor(0.)),\n",
              " (tensor([2.3603, 0.0623]), tensor(0.)),\n",
              " (tensor([ 3.2544, -0.3600]), tensor(0.)),\n",
              " (tensor([1.0733, 0.7115]), tensor(0.)),\n",
              " (tensor([0.2488, 0.9648]), tensor(0.)),\n",
              " (tensor([ 2.6355, -0.0796]), tensor(0.)),\n",
              " (tensor([ 4.5780, -0.6743]), tensor(0.)),\n",
              " (tensor([2.1949, 0.1502]), tensor(0.)),\n",
              " (tensor([ 2.6615, -0.0926]), tensor(0.)),\n",
              " (tensor([ 5.6149, -0.6718]), tensor(0.)),\n",
              " (tensor([ 6.2672, -0.6293]), tensor(0.)),\n",
              " (tensor([ 3.1841, -0.3318]), tensor(0.)),\n",
              " (tensor([1.8425, 0.3386]), tensor(0.)),\n",
              " (tensor([ 5.1363, -0.6911]), tensor(0.)),\n",
              " (tensor([ 4.5542, -0.6723]), tensor(0.)),\n",
              " (tensor([ 3.8759, -0.5605]), tensor(0.)),\n",
              " (tensor([ 5.4695, -0.6798]), tensor(0.)),\n",
              " (tensor([1.1704, 0.6698]), tensor(0.)),\n",
              " (tensor([ 3.3379, -0.3922]), tensor(0.)),\n",
              " (tensor([ 3.2171, -0.3451]), tensor(0.)),\n",
              " (tensor([ 3.4459, -0.4315]), tensor(0.)),\n",
              " (tensor([ 4.0899, -0.6076]), tensor(0.)),\n",
              " (tensor([ 6.0211, -0.6451]), tensor(0.)),\n",
              " (tensor([ 2.5842, -0.0537]), tensor(0.)),\n",
              " (tensor([ 5.6921, -0.6671]), tensor(0.)),\n",
              " (tensor([0.6006, 0.8809]), tensor(0.)),\n",
              " (tensor([ 3.3390, -0.3926]), tensor(0.)),\n",
              " (tensor([2.0555, 0.2249]), tensor(0.)),\n",
              " (tensor([ 3.9137, -0.5696]), tensor(0.)),\n",
              " (tensor([1.3681, 0.5792]), tensor(0.)),\n",
              " (tensor([ 2.9070, -0.2107]), tensor(0.)),\n",
              " (tensor([0.7907, 0.8199]), tensor(0.)),\n",
              " (tensor([1.5212, 0.5044]), tensor(0.)),\n",
              " (tensor([0.7864, 0.8214]), tensor(0.)),\n",
              " (tensor([ 3.6817, -0.5079]), tensor(0.)),\n",
              " (tensor([ 5.8882, -0.6542]), tensor(0.)),\n",
              " (tensor([ 3.6565, -0.5003]), tensor(0.)),\n",
              " (tensor([ 4.5690, -0.6735]), tensor(0.)),\n",
              " (tensor([ 4.9374, -0.6912]), tensor(0.)),\n",
              " (tensor([0.8359, 0.8040]), tensor(0.)),\n",
              " (tensor([ 3.4899, -0.4468]), tensor(0.)),\n",
              " (tensor([ 4.8944, -0.6904]), tensor(0.)),\n",
              " (tensor([ 4.5600, -0.6728]), tensor(0.)),\n",
              " (tensor([0.3760, 0.9390]), tensor(0.)),\n",
              " (tensor([ 5.3830, -0.6838]), tensor(0.)),\n",
              " (tensor([ 2.7655, -0.1437]), tensor(0.)),\n",
              " (tensor([ 5.7074, -0.6661]), tensor(0.)),\n",
              " (tensor([1.1740, 0.6682]), tensor(0.)),\n",
              " (tensor([ 3.6144, -0.4874]), tensor(0.)),\n",
              " (tensor([0.3827, 0.9375]), tensor(0.)),\n",
              " (tensor([ 6.2126, -0.6326]), tensor(0.)),\n",
              " (tensor([ 2.7433, -0.1330]), tensor(0.)),\n",
              " (tensor([2.4140, 0.0341]), tensor(0.)),\n",
              " (tensor([0.0882, 0.9899]), tensor(0.)),\n",
              " (tensor([1.6497, 0.4393]), tensor(0.)),\n",
              " (tensor([ 5.0426, -0.6919]), tensor(0.)),\n",
              " (tensor([0.8448, 0.8007]), tensor(0.)),\n",
              " (tensor([ 4.8984, -0.6905]), tensor(0.)),\n",
              " (tensor([ 3.1093, -0.3005]), tensor(0.)),\n",
              " (tensor([2.0672, 0.2187]), tensor(0.)),\n",
              " (tensor([0.5762, 0.8880]), tensor(0.)),\n",
              " (tensor([ 6.0664, -0.6421]), tensor(0.)),\n",
              " (tensor([1.5431, 0.4935]), tensor(0.)),\n",
              " (tensor([ 6.2000, -0.6334]), tensor(0.)),\n",
              " (tensor([ 3.4418, -0.4301]), tensor(0.)),\n",
              " (tensor([ 3.0662, -0.2821]), tensor(0.)),\n",
              " (tensor([1.8373, 0.3413]), tensor(0.)),\n",
              " (tensor([0.8063, 0.8145]), tensor(0.)),\n",
              " (tensor([ 4.2088, -0.6290]), tensor(0.)),\n",
              " (tensor([2.4758e+00, 1.8909e-03]), tensor(0.)),\n",
              " (tensor([1.7123, 0.4070]), tensor(0.)),\n",
              " (tensor([1.6816, 0.4229]), tensor(0.)),\n",
              " (tensor([1.4763, 0.5267]), tensor(0.)),\n",
              " (tensor([2.0453, 0.2304]), tensor(0.)),\n",
              " (tensor([0.8466, 0.8001]), tensor(0.)),\n",
              " (tensor([0.4757, 0.9151]), tensor(0.)),\n",
              " (tensor([ 4.5530, -0.6721]), tensor(0.)),\n",
              " (tensor([2.0263, 0.2406]), tensor(0.)),\n",
              " (tensor([1.0072, 0.7386]), tensor(0.)),\n",
              " (tensor([ 4.5410, -0.6711]), tensor(0.)),\n",
              " (tensor([ 4.3523, -0.6503]), tensor(0.)),\n",
              " (tensor([ 4.2135, -0.6298]), tensor(0.)),\n",
              " (tensor([ 3.7206, -0.5191]), tensor(0.)),\n",
              " (tensor([ 5.2646, -0.6882]), tensor(0.)),\n",
              " (tensor([ 4.5421, -0.6712]), tensor(0.)),\n",
              " (tensor([ 3.3395, -0.3928]), tensor(0.)),\n",
              " (tensor([ 5.3860, -0.6837]), tensor(0.)),\n",
              " (tensor([0.5546, 0.8941]), tensor(0.)),\n",
              " (tensor([0.9905, 0.7453]), tensor(0.)),\n",
              " (tensor([ 5.8678, -0.6556]), tensor(0.)),\n",
              " (tensor([ 4.3043, -0.6437]), tensor(0.)),\n",
              " (tensor([ 3.7696, -0.5328]), tensor(0.)),\n",
              " (tensor([ 5.2793, -0.6877]), tensor(0.)),\n",
              " (tensor([ 5.6980, -0.6667]), tensor(0.)),\n",
              " (tensor([1.0374, 0.7263]), tensor(0.)),\n",
              " (tensor([ 5.5570, -0.6752]), tensor(0.)),\n",
              " (tensor([ 6.2567, -0.6299]), tensor(0.)),\n",
              " (tensor([ 3.9828, -0.5854]), tensor(0.)),\n",
              " (tensor([ 2.6476, -0.0857]), tensor(0.)),\n",
              " (tensor([0.5449, 0.8967]), tensor(0.)),\n",
              " (tensor([ 3.6963, -0.5121]), tensor(0.)),\n",
              " (tensor([1.0433, 0.7239]), tensor(0.)),\n",
              " (tensor([ 5.2136, -0.6896]), tensor(0.)),\n",
              " (tensor([ 6.2678, -0.6292]), tensor(0.)),\n",
              " (tensor([ 2.8836, -0.1999]), tensor(0.)),\n",
              " (tensor([1.9285, 0.2929]), tensor(0.)),\n",
              " (tensor([ 4.3260, -0.6467]), tensor(0.)),\n",
              " (tensor([2.0688, 0.2178]), tensor(0.)),\n",
              " (tensor([ 6.1807, -0.6346]), tensor(0.)),\n",
              " (tensor([1.4556, 0.5369]), tensor(0.)),\n",
              " (tensor([1.2670, 0.6264]), tensor(0.)),\n",
              " (tensor([1.0838, 0.7070]), tensor(0.)),\n",
              " (tensor([1.6941, 0.4164]), tensor(0.)),\n",
              " (tensor([ 3.3212, -0.3859]), tensor(0.)),\n",
              " (tensor([ 4.5906, -0.6753]), tensor(0.)),\n",
              " (tensor([1.6041, 0.4627]), tensor(0.)),\n",
              " (tensor([0.8880, 0.7849]), tensor(0.)),\n",
              " (tensor([ 5.8166, -0.6590]), tensor(0.)),\n",
              " (tensor([0.8048, 0.8150]), tensor(0.)),\n",
              " (tensor([0.5303, 0.9008]), tensor(0.)),\n",
              " (tensor([ 4.5741, -0.6739]), tensor(0.)),\n",
              " (tensor([ 4.0220, -0.5939]), tensor(0.)),\n",
              " (tensor([ 2.6022, -0.0628]), tensor(0.)),\n",
              " (tensor([ 3.3870, -0.4104]), tensor(0.)),\n",
              " (tensor([ 5.3597, -0.6848]), tensor(0.)),\n",
              " (tensor([ 3.6186, -0.4887]), tensor(0.)),\n",
              " (tensor([ 2.6096, -0.0666]), tensor(0.)),\n",
              " (tensor([2.2234, 0.1350]), tensor(0.)),\n",
              " (tensor([ 2.6830, -0.1033]), tensor(0.)),\n",
              " (tensor([1.4377, 0.5456]), tensor(0.)),\n",
              " (tensor([ 3.2341, -0.3520]), tensor(0.)),\n",
              " (tensor([0.3988, 0.9338]), tensor(0.)),\n",
              " (tensor([0.2451, 0.9655]), tensor(0.)),\n",
              " (tensor([1.6362, 0.4463]), tensor(0.)),\n",
              " (tensor([0.4888, 0.9118]), tensor(0.)),\n",
              " (tensor([ 4.8231, -0.6884]), tensor(0.)),\n",
              " (tensor([1.8852, 0.3160]), tensor(0.)),\n",
              " (tensor([ 4.0361, -0.5968]), tensor(0.)),\n",
              " (tensor([ 3.5279, -0.4596]), tensor(0.)),\n",
              " (tensor([ 5.6824, -0.6677]), tensor(0.)),\n",
              " (tensor([0.1491, 0.9814]), tensor(0.)),\n",
              " (tensor([ 4.6063, -0.6765]), tensor(0.)),\n",
              " (tensor([1.5401, 0.4950]), tensor(0.)),\n",
              " (tensor([1.3308, 0.5968]), tensor(0.)),\n",
              " (tensor([ 5.4576, -0.6804]), tensor(0.)),\n",
              " (tensor([0.1800, 0.9766]), tensor(0.)),\n",
              " (tensor([ 4.4187, -0.6585]), tensor(0.)),\n",
              " (tensor([ 6.0511, -0.6431]), tensor(0.)),\n",
              " (tensor([1.3245, 0.5998]), tensor(0.)),\n",
              " (tensor([1.4089, 0.5596]), tensor(0.)),\n",
              " (tensor([ 4.6193, -0.6775]), tensor(0.)),\n",
              " (tensor([1.3702, 0.5782]), tensor(0.)),\n",
              " (tensor([ 3.4904, -0.4470]), tensor(0.)),\n",
              " (tensor([ 3.4141, -0.4203]), tensor(0.)),\n",
              " (tensor([ 2.9030, -0.2089]), tensor(0.)),\n",
              " (tensor([1.2692, 0.6254]), tensor(0.)),\n",
              " (tensor([ 3.9112, -0.5690]), tensor(0.)),\n",
              " (tensor([2.4083, 0.0371]), tensor(0.)),\n",
              " (tensor([ 4.4977, -0.6670]), tensor(0.)),\n",
              " (tensor([ 6.0426, -0.6437]), tensor(0.)),\n",
              " (tensor([ 5.4999, -0.6783]), tensor(0.)),\n",
              " (tensor([1.8082, 0.3567]), tensor(0.)),\n",
              " (tensor([1.2172, 0.6490]), tensor(0.)),\n",
              " (tensor([ 4.8579, -0.6895]), tensor(0.)),\n",
              " (tensor([0.1637, 0.9792]), tensor(0.)),\n",
              " (tensor([1.3872, 0.5700]), tensor(0.)),\n",
              " (tensor([0.5272, 0.9016]), tensor(0.)),\n",
              " (tensor([ 4.2872, -0.6412]), tensor(0.)),\n",
              " (tensor([ 2.8965, -0.2059]), tensor(0.)),\n",
              " (tensor([2.2378, 0.1273]), tensor(0.)),\n",
              " (tensor([2.4755e+00, 2.0746e-03]), tensor(0.)),\n",
              " (tensor([2.4694, 0.0052]), tensor(0.)),\n",
              " (tensor([2.4656, 0.0072]), tensor(0.)),\n",
              " (tensor([ 4.2690, -0.6385]), tensor(0.)),\n",
              " (tensor([1.0667, 0.7142]), tensor(0.)),\n",
              " (tensor([ 2.7344, -0.1286]), tensor(0.)),\n",
              " (tensor([ 4.6876, -0.6820]), tensor(0.)),\n",
              " (tensor([ 4.9456, -0.6913]), tensor(0.)),\n",
              " (tensor([ 5.3457, -0.6854]), tensor(0.)),\n",
              " (tensor([2.3604, 0.0623]), tensor(0.)),\n",
              " (tensor([ 4.0744, -0.6046]), tensor(0.)),\n",
              " (tensor([2.2454, 0.1233]), tensor(0.)),\n",
              " (tensor([ 4.8744, -0.6899]), tensor(0.)),\n",
              " (tensor([ 2.6628, -0.0933]), tensor(0.)),\n",
              " (tensor([ 3.6652, -0.5029]), tensor(0.)),\n",
              " (tensor([2.2267, 0.1332]), tensor(0.)),\n",
              " (tensor([ 4.9187, -0.6909]), tensor(0.)),\n",
              " (tensor([2.2620, 0.1144]), tensor(0.)),\n",
              " (tensor([ 3.8909, -0.5642]), tensor(0.)),\n",
              " (tensor([ 2.6162, -0.0699]), tensor(0.)),\n",
              " (tensor([ 3.1082, -0.3001]), tensor(0.)),\n",
              " (tensor([ 2.6944, -0.1089]), tensor(0.)),\n",
              " (tensor([ 3.1694, -0.3257]), tensor(0.)),\n",
              " (tensor([ 3.2768, -0.3688]), tensor(0.)),\n",
              " (tensor([1.4612, 0.5341]), tensor(0.)),\n",
              " (tensor([0.6230, 0.8743]), tensor(0.)),\n",
              " (tensor([2.4445, 0.0182]), tensor(0.)),\n",
              " (tensor([ 4.3931, -0.6554]), tensor(0.)),\n",
              " (tensor([ 2.9430, -0.2273]), tensor(0.)),\n",
              " (tensor([ 3.8966, -0.5655]), tensor(0.)),\n",
              " (tensor([ 2.8886, -0.2022]), tensor(0.)),\n",
              " (tensor([1.0953, 0.7022]), tensor(0.)),\n",
              " (tensor([ 5.5642, -0.6748]), tensor(0.)),\n",
              " (tensor([ 4.3514, -0.6501]), tensor(0.)),\n",
              " (tensor([ 3.6273, -0.4914]), tensor(0.)),\n",
              " (tensor([ 4.0330, -0.5962]), tensor(0.)),\n",
              " (tensor([1.9472, 0.2829]), tensor(0.)),\n",
              " (tensor([1.3777, 0.5746]), tensor(0.)),\n",
              " (tensor([ 2.5892, -0.0563]), tensor(0.)),\n",
              " (tensor([ 5.2374, -0.6890]), tensor(0.)),\n",
              " (tensor([ 3.3974, -0.4142]), tensor(0.)),\n",
              " (tensor([1.6715, 0.4281]), tensor(0.)),\n",
              " (tensor([0.8339, 0.8047]), tensor(0.)),\n",
              " (tensor([1.9461, 0.2835]), tensor(0.)),\n",
              " (tensor([0.1707, 0.9781]), tensor(0.)),\n",
              " (tensor([0.0990, 0.9885]), tensor(0.)),\n",
              " (tensor([ 5.5289, -0.6767]), tensor(0.)),\n",
              " (tensor([ 6.1121, -0.6391]), tensor(0.)),\n",
              " (tensor([ 2.7458, -0.1342]), tensor(0.)),\n",
              " (tensor([0.1313, 0.9840]), tensor(0.)),\n",
              " (tensor([ 5.9559, -0.6496]), tensor(0.)),\n",
              " (tensor([ 3.8303, -0.5490]), tensor(0.)),\n",
              " (tensor([ 3.5115, -0.4541]), tensor(0.)),\n",
              " (tensor([0.0944, 0.9891]), tensor(0.)),\n",
              " (tensor([ 5.7834, -0.6612]), tensor(0.)),\n",
              " (tensor([1.8728, 0.3225]), tensor(0.)),\n",
              " (tensor([ 3.3211, -0.3859]), tensor(0.)),\n",
              " (tensor([1.0813, 0.7081]), tensor(0.)),\n",
              " (tensor([ 5.7155, -0.6656]), tensor(0.)),\n",
              " (tensor([1.7750, 0.3742]), tensor(0.)),\n",
              " (tensor([ 2.5059, -0.0136]), tensor(0.)),\n",
              " (tensor([ 4.3726, -0.6529]), tensor(0.)),\n",
              " (tensor([ 2.7401, -0.1314]), tensor(0.)),\n",
              " (tensor([ 5.4655, -0.6800]), tensor(0.)),\n",
              " (tensor([0.1738, 0.9776]), tensor(0.)),\n",
              " (tensor([ 3.7011, -0.5135]), tensor(0.)),\n",
              " (tensor([ 2.5349, -0.0286]), tensor(0.)),\n",
              " (tensor([0.6521, 0.8654]), tensor(0.)),\n",
              " (tensor([0.7221, 0.8431]), tensor(0.)),\n",
              " (tensor([ 5.2554, -0.6885]), tensor(0.)),\n",
              " (tensor([ 3.1452, -0.3157]), tensor(0.)),\n",
              " (tensor([1.9264, 0.2940]), tensor(0.)),\n",
              " (tensor([0.0980, 0.9886]), tensor(0.)),\n",
              " (tensor([ 4.6624, -0.6805]), tensor(0.)),\n",
              " (tensor([1.8740, 0.3219]), tensor(0.)),\n",
              " (tensor([ 4.1212, -0.6136]), tensor(0.)),\n",
              " (tensor([2.1299, 0.1850]), tensor(0.)),\n",
              " (tensor([ 5.6169, -0.6717]), tensor(0.)),\n",
              " (tensor([ 4.2073, -0.6287]), tensor(0.)),\n",
              " (tensor([0.7087, 0.8475]), tensor(0.)),\n",
              " (tensor([ 3.6809, -0.5076]), tensor(0.)),\n",
              " (tensor([ 4.7639, -0.6860]), tensor(0.)),\n",
              " (tensor([0.7633, 0.8294]), tensor(0.)),\n",
              " (tensor([0.8824, 0.7870]), tensor(0.)),\n",
              " (tensor([ 5.5645, -0.6748]), tensor(0.)),\n",
              " (tensor([ 6.1821, -0.6345]), tensor(0.)),\n",
              " (tensor([ 4.9780, -0.6916]), tensor(0.)),\n",
              " (tensor([0.1755, 0.9773]), tensor(0.)),\n",
              " (tensor([ 3.3822, -0.4087]), tensor(0.)),\n",
              " (tensor([ 4.6133, -0.6770]), tensor(0.)),\n",
              " (tensor([0.0023, 0.9998]), tensor(0.)),\n",
              " (tensor([0.8636, 0.7939]), tensor(0.)),\n",
              " (tensor([ 5.3247, -0.6862]), tensor(0.)),\n",
              " (tensor([ 4.9765, -0.6916]), tensor(0.)),\n",
              " (tensor([2.3154, 0.0861]), tensor(0.)),\n",
              " (tensor([ 3.1737, -0.3275]), tensor(0.)),\n",
              " (tensor([ 4.2233, -0.6314]), tensor(0.)),\n",
              " (tensor([0.5684, 0.8902]), tensor(0.)),\n",
              " (tensor([ 4.4338, -0.6602]), tensor(0.)),\n",
              " (tensor([ 5.1487, -0.6909]), tensor(0.)),\n",
              " (tensor([2.1994, 0.1478]), tensor(0.)),\n",
              " (tensor([1.4392, 0.5449]), tensor(0.)),\n",
              " (tensor([ 4.8471, -0.6891]), tensor(0.)),\n",
              " (tensor([ 3.3065, -0.3803]), tensor(0.)),\n",
              " (tensor([ 4.4737, -0.6646]), tensor(0.)),\n",
              " (tensor([1.6663, 0.4308]), tensor(0.)),\n",
              " (tensor([ 3.1925, -0.3352]), tensor(0.)),\n",
              " (tensor([2.1778, 0.1594]), tensor(0.)),\n",
              " (tensor([1.9564, 0.2780]), tensor(0.)),\n",
              " (tensor([ 5.2224, -0.6894]), tensor(0.)),\n",
              " (tensor([ 4.0800, -0.6057]), tensor(0.)),\n",
              " (tensor([0.4497, 0.9217]), tensor(0.)),\n",
              " (tensor([ 3.7462, -0.5264]), tensor(0.)),\n",
              " (tensor([0.3220, 0.9506]), tensor(0.)),\n",
              " (tensor([ 4.0692, -0.6036]), tensor(0.)),\n",
              " (tensor([ 6.2001, -0.6334]), tensor(0.)),\n",
              " (tensor([ 6.2453, -0.6306]), tensor(0.)),\n",
              " (tensor([1.0209, 0.7330]), tensor(0.)),\n",
              " (tensor([ 6.0117, -0.6458]), tensor(0.)),\n",
              " (tensor([ 3.3703, -0.4043]), tensor(0.)),\n",
              " (tensor([ 5.7151, -0.6656]), tensor(0.)),\n",
              " (tensor([ 4.4215, -0.6588]), tensor(0.)),\n",
              " (tensor([1.3771, 0.5749]), tensor(0.)),\n",
              " (tensor([0.8633, 0.7940]), tensor(0.)),\n",
              " (tensor([ 4.9380, -0.6912]), tensor(0.)),\n",
              " (tensor([0.9310, 0.7686]), tensor(0.)),\n",
              " (tensor([1.6559, 0.4361]), tensor(0.)),\n",
              " (tensor([ 2.9098, -0.2120]), tensor(0.)),\n",
              " (tensor([ 3.8167, -0.5454]), tensor(0.)),\n",
              " (tensor([1.7280, 0.3987]), tensor(0.)),\n",
              " (tensor([ 3.1127, -0.3020]), tensor(0.)),\n",
              " (tensor([0.0969, 0.9888]), tensor(0.)),\n",
              " (tensor([0.6716, 0.8593]), tensor(0.)),\n",
              " (tensor([ 2.9599, -0.2350]), tensor(0.)),\n",
              " (tensor([ 5.6646, -0.6688]), tensor(0.)),\n",
              " (tensor([ 4.4732, -0.6645]), tensor(0.)),\n",
              " (tensor([ 2.5422, -0.0323]), tensor(0.)),\n",
              " (tensor([2.0999, 0.2011]), tensor(0.)),\n",
              " (tensor([ 6.2269, -0.6317]), tensor(0.)),\n",
              " (tensor([2.2226, 0.1354]), tensor(0.)),\n",
              " (tensor([0.6955, 0.8517]), tensor(0.)),\n",
              " (tensor([ 3.2512, -0.3588]), tensor(0.)),\n",
              " (tensor([ 5.1891, -0.6901]), tensor(0.)),\n",
              " (tensor([1.2044, 0.6548]), tensor(0.)),\n",
              " (tensor([ 5.0763, -0.6917]), tensor(0.)),\n",
              " (tensor([0.5851, 0.8854]), tensor(0.)),\n",
              " (tensor([0.3538, 0.9439]), tensor(0.)),\n",
              " (tensor([0.1021, 0.9880]), tensor(0.)),\n",
              " (tensor([ 5.3781, -0.6840]), tensor(0.)),\n",
              " (tensor([ 5.5930, -0.6731]), tensor(0.)),\n",
              " (tensor([1.4395, 0.5448]), tensor(0.)),\n",
              " (tensor([1.1263, 0.6890]), tensor(0.)),\n",
              " (tensor([ 2.9599, -0.2349]), tensor(0.)),\n",
              " (tensor([ 4.6141, -0.6771]), tensor(0.)),\n",
              " (tensor([ 5.0091, -0.6918]), tensor(0.)),\n",
              " (tensor([ 3.9718, -0.5830]), tensor(0.)),\n",
              " (tensor([ 4.6448, -0.6793]), tensor(0.)),\n",
              " (tensor([ 4.9536, -0.6914]), tensor(0.)),\n",
              " (tensor([ 6.0374, -0.6440]), tensor(0.)),\n",
              " (tensor([ 3.8482, -0.5535]), tensor(0.)),\n",
              " (tensor([2.3972, 0.0429]), tensor(0.)),\n",
              " (tensor([ 3.3685, -0.4036]), tensor(0.)),\n",
              " (tensor([ 4.0988, -0.6093]), tensor(0.)),\n",
              " (tensor([ 3.3611, -0.4009]), tensor(0.)),\n",
              " (tensor([2.4086, 0.0370]), tensor(0.)),\n",
              " (tensor([ 5.8810, -0.6547]), tensor(0.)),\n",
              " (tensor([ 6.1970, -0.6336]), tensor(0.)),\n",
              " (tensor([ 4.7386, -0.6848]), tensor(0.)),\n",
              " (tensor([ 4.7814, -0.6868]), tensor(0.)),\n",
              " (tensor([2.1794, 0.1585]), tensor(0.)),\n",
              " (tensor([ 4.4251, -0.6592]), tensor(0.)),\n",
              " (tensor([ 5.1450, -0.6910]), tensor(0.)),\n",
              " (tensor([ 4.3022, -0.6434]), tensor(0.)),\n",
              " (tensor([2.3447, 0.0705]), tensor(0.)),\n",
              " (tensor([1.0726, 0.7117]), tensor(0.)),\n",
              " (tensor([0.7556, 0.8320]), tensor(0.)),\n",
              " (tensor([ 4.0123, -0.5918]), tensor(0.)),\n",
              " (tensor([ 3.2464, -0.3569]), tensor(0.)),\n",
              " (tensor([ 5.2652, -0.6882]), tensor(0.)),\n",
              " (tensor([ 6.2677, -0.6292]), tensor(0.)),\n",
              " (tensor([0.5638, 0.8915]), tensor(0.)),\n",
              " (tensor([ 5.1642, -0.6906]), tensor(0.)),\n",
              " (tensor([ 3.7494, -0.5273]), tensor(0.)),\n",
              " (tensor([0.8920, 0.7834]), tensor(0.)),\n",
              " (tensor([1.8126, 0.3544]), tensor(0.)),\n",
              " (tensor([0.1429, 0.9823]), tensor(0.)),\n",
              " (tensor([ 3.5332, -0.4613]), tensor(0.)),\n",
              " (tensor([ 3.6729, -0.5052]), tensor(0.)),\n",
              " (tensor([ 4.9154, -0.6908]), tensor(0.)),\n",
              " (tensor([ 2.8749, -0.1958]), tensor(0.)),\n",
              " (tensor([1.7720, 0.3758]), tensor(0.)),\n",
              " (tensor([ 5.2024, -0.6898]), tensor(0.)),\n",
              " (tensor([ 4.7815, -0.6868]), tensor(0.)),\n",
              " (tensor([1.7615, 0.3813]), tensor(0.)),\n",
              " (tensor([ 4.1195, -0.6133]), tensor(0.)),\n",
              " (tensor([ 3.1815, -0.3307]), tensor(0.)),\n",
              " (tensor([1.3195, 0.6021]), tensor(0.)),\n",
              " (tensor([0.9334, 0.7676]), tensor(0.)),\n",
              " (tensor([ 4.3252, -0.6466]), tensor(0.)),\n",
              " (tensor([ 4.1711, -0.6226]), tensor(0.)),\n",
              " (tensor([1.5877, 0.4710]), tensor(0.)),\n",
              " (tensor([ 3.0023, -0.2540]), tensor(0.)),\n",
              " (tensor([ 5.9664, -0.6489]), tensor(0.)),\n",
              " (tensor([ 6.0321, -0.6444]), tensor(0.)),\n",
              " (tensor([ 3.9796, -0.5847]), tensor(0.)),\n",
              " (tensor([0.8645, 0.7936]), tensor(0.)),\n",
              " (tensor([0.7836, 0.8224]), tensor(0.)),\n",
              " (tensor([0.6995, 0.8505]), tensor(0.)),\n",
              " (tensor([ 5.3725, -0.6843]), tensor(0.)),\n",
              " (tensor([ 3.7748, -0.5342]), tensor(0.)),\n",
              " (tensor([ 5.9812, -0.6478]), tensor(0.)),\n",
              " (tensor([0.5391, 0.8984]), tensor(0.)),\n",
              " (tensor([1.3633, 0.5814]), tensor(0.)),\n",
              " (tensor([ 2.5297, -0.0259]), tensor(0.)),\n",
              " (tensor([ 3.1853, -0.3322]), tensor(0.)),\n",
              " (tensor([ 5.9511, -0.6499]), tensor(0.)),\n",
              " (tensor([2.2709, 0.1097]), tensor(0.)),\n",
              " (tensor([ 4.9717, -0.6916]), tensor(0.)),\n",
              " (tensor([0.3332, 0.9483]), tensor(0.)),\n",
              " (tensor([ 4.7814, -0.6868]), tensor(0.)),\n",
              " (tensor([2.4006, 0.0412]), tensor(0.)),\n",
              " (tensor([0.8677, 0.7924]), tensor(0.)),\n",
              " (tensor([ 3.2458, -0.3566]), tensor(0.)),\n",
              " (tensor([1.5738, 0.4780]), tensor(0.)),\n",
              " (tensor([ 4.2367, -0.6335]), tensor(0.)),\n",
              " (tensor([2.0031, 0.2530]), tensor(0.)),\n",
              " (tensor([ 6.0389, -0.6439]), tensor(0.)),\n",
              " (tensor([ 5.1543, -0.6908]), tensor(0.)),\n",
              " (tensor([ 3.2071, -0.3411]), tensor(0.)),\n",
              " (tensor([ 3.3704, -0.4043]), tensor(0.)),\n",
              " (tensor([ 3.0300, -0.2663]), tensor(0.)),\n",
              " (tensor([ 6.1340, -0.6376]), tensor(0.)),\n",
              " (tensor([ 3.4908, -0.4471]), tensor(0.)),\n",
              " (tensor([ 3.5295, -0.4601]), tensor(0.)),\n",
              " (tensor([ 4.7723, -0.6864]), tensor(0.)),\n",
              " (tensor([0.7083, 0.8476]), tensor(0.)),\n",
              " (tensor([1.4587, 0.5354]), tensor(0.)),\n",
              " (tensor([ 3.3601, -0.4005]), tensor(0.)),\n",
              " (tensor([ 3.3774, -0.4069]), tensor(0.)),\n",
              " (tensor([1.2893, 0.6161]), tensor(0.)),\n",
              " (tensor([0.4996, 0.9090]), tensor(0.)),\n",
              " (tensor([ 5.4291, -0.6818]), tensor(0.)),\n",
              " (tensor([ 3.6506, -0.4986]), tensor(0.)),\n",
              " (tensor([ 3.1408, -0.3138]), tensor(0.)),\n",
              " (tensor([0.1935, 0.9744]), tensor(0.)),\n",
              " (tensor([1.4660, 0.5318]), tensor(0.)),\n",
              " (tensor([ 6.0728, -0.6417]), tensor(0.)),\n",
              " (tensor([ 2.5048, -0.0131]), tensor(0.)),\n",
              " (tensor([ 2.8854, -0.2007]), tensor(0.)),\n",
              " (tensor([ 4.1642, -0.6214]), tensor(0.)),\n",
              " (tensor([ 3.1802, -0.3302]), tensor(0.)),\n",
              " (tensor([ 5.1576, -0.6907]), tensor(0.)),\n",
              " (tensor([ 2.9317, -0.2221]), tensor(0.)),\n",
              " (tensor([ 3.0881, -0.2915]), tensor(0.)),\n",
              " (tensor([1.0975, 0.7012]), tensor(0.)),\n",
              " (tensor([0.0845, 0.9904]), tensor(0.)),\n",
              " (tensor([0.3155, 0.9519]), tensor(0.)),\n",
              " (tensor([ 3.3926, -0.4125]), tensor(0.)),\n",
              " (tensor([ 3.5849, -0.4781]), tensor(0.)),\n",
              " (tensor([0.4426, 0.9234]), tensor(0.)),\n",
              " (tensor([ 4.0520, -0.6001]), tensor(0.)),\n",
              " (tensor([0.0377, 0.9960]), tensor(0.)),\n",
              " (tensor([ 5.1057, -0.6915]), tensor(0.)),\n",
              " (tensor([0.9337, 0.7675]), tensor(0.)),\n",
              " (tensor([ 3.9947, -0.5881]), tensor(0.)),\n",
              " (tensor([ 6.2804, -0.6285]), tensor(0.)),\n",
              " (tensor([1.2620, 0.6287]), tensor(0.)),\n",
              " (tensor([0.5462, 0.8964]), tensor(0.)),\n",
              " (tensor([ 5.6654, -0.6688]), tensor(0.)),\n",
              " (tensor([ 6.2094, -0.6328]), tensor(0.)),\n",
              " (tensor([ 6.1962, -0.6336]), tensor(0.)),\n",
              " (tensor([ 4.3937, -0.6555]), tensor(0.)),\n",
              " (tensor([ 5.5606, -0.6750]), tensor(0.)),\n",
              " (tensor([ 5.7764, -0.6617]), tensor(0.)),\n",
              " (tensor([ 3.3687, -0.4037]), tensor(0.)),\n",
              " (tensor([ 4.1733, -0.6230]), tensor(0.)),\n",
              " (tensor([ 6.0425, -0.6437]), tensor(0.)),\n",
              " (tensor([ 5.5062, -0.6780]), tensor(0.)),\n",
              " (tensor([2.3725, 0.0559]), tensor(0.)),\n",
              " (tensor([ 3.0176, -0.2608]), tensor(0.)),\n",
              " (tensor([0.2733, 0.9603]), tensor(0.)),\n",
              " (tensor([1.8262, 0.3472]), tensor(0.)),\n",
              " (tensor([0.6689, 0.8602]), tensor(0.)),\n",
              " (tensor([1.3333, 0.5956]), tensor(0.)),\n",
              " (tensor([ 4.6533, -0.6799]), tensor(0.)),\n",
              " (tensor([0.7878, 0.8210]), tensor(0.)),\n",
              " (tensor([ 2.4961, -0.0086]), tensor(0.)),\n",
              " (tensor([ 2.7281, -0.1255]), tensor(0.)),\n",
              " (tensor([ 6.0605, -0.6425]), tensor(0.)),\n",
              " (tensor([1.2367, 0.6402]), tensor(0.)),\n",
              " (tensor([ 4.2932, -0.6421]), tensor(0.)),\n",
              " (tensor([ 4.5199, -0.6692]), tensor(0.)),\n",
              " (tensor([0.2851, 0.9580]), tensor(0.)),\n",
              " (tensor([0.1875, 0.9754]), tensor(0.)),\n",
              " (tensor([ 4.6178, -0.6774]), tensor(0.)),\n",
              " (tensor([1.9814, 0.2646]), tensor(0.)),\n",
              " (tensor([ 4.4581, -0.6629]), tensor(0.)),\n",
              " (tensor([ 3.0026, -0.2541]), tensor(0.)),\n",
              " (tensor([ 4.2007, -0.6276]), tensor(0.)),\n",
              " (tensor([ 5.8347, -0.6578]), tensor(0.)),\n",
              " (tensor([ 5.5772, -0.6740]), tensor(0.)),\n",
              " (tensor([ 4.8931, -0.6904]), tensor(0.)),\n",
              " (tensor([1.4128, 0.5577]), tensor(0.)),\n",
              " (tensor([ 6.2716, -0.6290]), tensor(0.)),\n",
              " (tensor([ 5.4012, -0.6831]), tensor(0.)),\n",
              " (tensor([1.5003, 0.5148]), tensor(0.)),\n",
              " (tensor([ 4.6820, -0.6817]), tensor(0.)),\n",
              " (tensor([ 3.4051, -0.4170]), tensor(0.)),\n",
              " (tensor([0.5035, 0.9079]), tensor(0.)),\n",
              " (tensor([ 5.1951, -0.6900]), tensor(0.)),\n",
              " (tensor([ 5.6931, -0.6670]), tensor(0.)),\n",
              " (tensor([ 2.5143, -0.0180]), tensor(0.)),\n",
              " (tensor([ 4.4823, -0.6654]), tensor(0.)),\n",
              " (tensor([0.2545, 0.9638]), tensor(0.)),\n",
              " (tensor([ 2.7686, -0.1453]), tensor(0.)),\n",
              " (tensor([ 4.9121, -0.6907]), tensor(0.)),\n",
              " (tensor([ 5.5043, -0.6781]), tensor(0.)),\n",
              " (tensor([ 4.9813, -0.6917]), tensor(0.)),\n",
              " (tensor([ 2.9446, -0.2280]), tensor(0.)),\n",
              " (tensor([1.2170, 0.6491]), tensor(0.)),\n",
              " (tensor([ 5.1080, -0.6914]), tensor(0.)),\n",
              " (tensor([1.8734, 0.3222]), tensor(0.)),\n",
              " (tensor([2.4237, 0.0290]), tensor(0.)),\n",
              " (tensor([ 4.2268, -0.6319]), tensor(0.)),\n",
              " (tensor([1.6403, 0.4441]), tensor(0.)),\n",
              " (tensor([ 6.1656, -0.6356]), tensor(0.)),\n",
              " (tensor([ 4.9208, -0.6909]), tensor(0.)),\n",
              " (tensor([ 3.0501, -0.2751]), tensor(0.)),\n",
              " (tensor([ 3.6206, -0.4894]), tensor(0.)),\n",
              " (tensor([0.3700, 0.9403]), tensor(0.)),\n",
              " (tensor([ 2.9349, -0.2235]), tensor(0.)),\n",
              " (tensor([ 4.9864, -0.6917]), tensor(0.)),\n",
              " (tensor([2.4299, 0.0258]), tensor(0.)),\n",
              " (tensor([0.4826, 0.9134]), tensor(0.)),\n",
              " (tensor([ 4.2198, -0.6308]), tensor(0.)),\n",
              " (tensor([1.1255, 0.6893]), tensor(0.)),\n",
              " (tensor([ 5.7730, -0.6619]), tensor(0.)),\n",
              " (tensor([ 5.1517, -0.6908]), tensor(0.)),\n",
              " (tensor([0.1449, 0.9820]), tensor(0.)),\n",
              " (tensor([ 4.7754, -0.6865]), tensor(0.)),\n",
              " (tensor([0.8880, 0.7849]), tensor(0.)),\n",
              " (tensor([ 2.8967, -0.2060]), tensor(0.)),\n",
              " (tensor([ 4.3586, -0.6511]), tensor(0.)),\n",
              " (tensor([0.4035, 0.9327]), tensor(0.)),\n",
              " (tensor([0.9948, 0.7436]), tensor(0.)),\n",
              " (tensor([ 6.1202, -0.6385]), tensor(0.)),\n",
              " (tensor([ 4.1396, -0.6170]), tensor(0.)),\n",
              " (tensor([ 3.1145, -0.3027]), tensor(0.)),\n",
              " (tensor([2.1185, 0.1911]), tensor(0.)),\n",
              " (tensor([1.6213, 0.4539]), tensor(0.)),\n",
              " (tensor([1.9086, 0.3035]), tensor(0.)),\n",
              " (tensor([0.6050, 0.8796]), tensor(0.)),\n",
              " (tensor([2.1084, 0.1965]), tensor(0.)),\n",
              " (tensor([ 3.6422, -0.4960]), tensor(0.)),\n",
              " (tensor([0.1217, 0.9854]), tensor(0.)),\n",
              " (tensor([ 5.9514, -0.6499]), tensor(0.)),\n",
              " (tensor([ 3.9132, -0.5695]), tensor(0.)),\n",
              " (tensor([ 4.8219, -0.6883]), tensor(0.)),\n",
              " (tensor([0.9095, 0.7768]), tensor(0.)),\n",
              " (tensor([ 3.0345, -0.2682]), tensor(0.)),\n",
              " (tensor([ 3.0591, -0.2790]), tensor(0.)),\n",
              " (tensor([ 3.9186, -0.5708]), tensor(0.)),\n",
              " (tensor([ 3.7353, -0.5233]), tensor(0.)),\n",
              " (tensor([ 5.0827, -0.6917]), tensor(0.)),\n",
              " (tensor([ 4.6767, -0.6814]), tensor(0.)),\n",
              " (tensor([1.9350, 0.2894]), tensor(0.)),\n",
              " (tensor([1.7433, 0.3908]), tensor(0.)),\n",
              " (tensor([1.2607, 0.6293]), tensor(0.)),\n",
              " (tensor([ 3.5633, -0.4712]), tensor(0.)),\n",
              " (tensor([0.7770, 0.8247]), tensor(0.)),\n",
              " (tensor([ 2.7692, -0.1456]), tensor(0.)),\n",
              " (tensor([1.2530, 0.6328]), tensor(0.)),\n",
              " (tensor([ 4.0120, -0.5918]), tensor(0.)),\n",
              " (tensor([2.0489, 0.2284]), tensor(0.)),\n",
              " (tensor([0.0472, 0.9949]), tensor(0.)),\n",
              " (tensor([ 5.6798, -0.6679]), tensor(0.)),\n",
              " (tensor([ 5.7086, -0.6661]), tensor(0.)),\n",
              " (tensor([ 3.2659, -0.3645]), tensor(0.)),\n",
              " (tensor([0.5176, 0.9042]), tensor(0.)),\n",
              " (tensor([ 4.6900, -0.6822]), tensor(0.)),\n",
              " (tensor([1.6628, 0.4325]), tensor(0.)),\n",
              " (tensor([ 6.1354, -0.6375]), tensor(0.)),\n",
              " (tensor([ 5.5240, -0.6770]), tensor(0.)),\n",
              " (tensor([ 5.3243, -0.6862]), tensor(0.)),\n",
              " (tensor([ 3.4452, -0.4313]), tensor(0.)),\n",
              " (tensor([0.9600, 0.7573]), tensor(0.)),\n",
              " (tensor([ 5.8417, -0.6573]), tensor(0.)),\n",
              " (tensor([ 5.5534, -0.6754]), tensor(0.)),\n",
              " (tensor([ 5.4972, -0.6784]), tensor(0.)),\n",
              " (tensor([ 5.5645, -0.6748]), tensor(0.)),\n",
              " (tensor([1.1066, 0.6974]), tensor(0.)),\n",
              " (tensor([ 2.7203, -0.1217]), tensor(0.)),\n",
              " (tensor([0.9414, 0.7646]), tensor(0.)),\n",
              " (tensor([ 4.5208, -0.6692]), tensor(0.)),\n",
              " (tensor([0.6190, 0.8755]), tensor(0.)),\n",
              " (tensor([ 3.1012, -0.2971]), tensor(0.)),\n",
              " (tensor([ 2.6182, -0.0709]), tensor(0.)),\n",
              " (tensor([ 3.7056, -0.5148]), tensor(0.)),\n",
              " (tensor([ 2.7255, -0.1243]), tensor(0.)),\n",
              " (tensor([ 3.4471, -0.4320]), tensor(0.)),\n",
              " (tensor([ 3.5999, -0.4829]), tensor(0.)),\n",
              " (tensor([ 3.3194, -0.3852]), tensor(0.)),\n",
              " (tensor([ 4.1985, -0.6273]), tensor(0.)),\n",
              " (tensor([1.9869, 0.2617]), tensor(0.)),\n",
              " (tensor([2.3357, 0.0753]), tensor(0.)),\n",
              " (tensor([ 4.0200, -0.5935]), tensor(0.)),\n",
              " (tensor([1.1168, 0.6930]), tensor(0.)),\n",
              " (tensor([ 5.8461, -0.6570]), tensor(0.)),\n",
              " (tensor([ 3.7028, -0.5140]), tensor(0.)),\n",
              " (tensor([ 4.0424, -0.5981]), tensor(0.)),\n",
              " (tensor([0.0204, 0.9979]), tensor(0.)),\n",
              " (tensor([ 2.6434, -0.0836]), tensor(0.)),\n",
              " (tensor([1.5919, 0.4689]), tensor(0.)),\n",
              " (tensor([ 2.7618, -0.1419]), tensor(0.)),\n",
              " (tensor([2.3085, 0.0897]), tensor(0.)),\n",
              " (tensor([ 3.7136, -0.5171]), tensor(0.)),\n",
              " (tensor([ 4.0136, -0.5921]), tensor(0.)),\n",
              " (tensor([ 5.1953, -0.6900]), tensor(0.)),\n",
              " (tensor([ 4.1827, -0.6246]), tensor(0.)),\n",
              " (tensor([ 3.5362, -0.4623]), tensor(0.)),\n",
              " (tensor([ 5.2799, -0.6877]), tensor(0.)),\n",
              " (tensor([0.7006, 0.8501]), tensor(0.)),\n",
              " (tensor([0.6230, 0.8742]), tensor(0.)),\n",
              " (tensor([ 2.8027, -0.1617]), tensor(0.)),\n",
              " (tensor([0.7592, 0.8308]), tensor(0.)),\n",
              " (tensor([1.4781, 0.5258]), tensor(0.)),\n",
              " (tensor([1.0646, 0.7151]), tensor(0.)),\n",
              " (tensor([ 3.3390, -0.3926]), tensor(0.)),\n",
              " (tensor([ 4.7740, -0.6865]), tensor(0.)),\n",
              " (tensor([ 3.5856, -0.4784]), tensor(0.)),\n",
              " (tensor([ 2.9403, -0.2260]), tensor(0.)),\n",
              " (tensor([1.9699, 0.2708]), tensor(0.)),\n",
              " (tensor([ 2.8118, -0.1660]), tensor(0.)),\n",
              " (tensor([ 3.9118, -0.5692]), tensor(0.)),\n",
              " (tensor([2.3506, 0.0675]), tensor(0.)),\n",
              " (tensor([ 2.6514, -0.0876]), tensor(0.)),\n",
              " (tensor([0.6916, 0.8530]), tensor(0.)),\n",
              " (tensor([0.4918, 0.9110]), tensor(0.)),\n",
              " (tensor([ 5.3467, -0.6853]), tensor(0.)),\n",
              " (tensor([1.9503, 0.2812]), tensor(0.)),\n",
              " (tensor([2.2594, 0.1158]), tensor(0.)),\n",
              " (tensor([1.8973, 0.3095]), tensor(0.)),\n",
              " (tensor([ 3.9600, -0.5803]), tensor(0.)),\n",
              " (tensor([ 2.9923, -0.2495]), tensor(0.)),\n",
              " (tensor([0.9826, 0.7484]), tensor(0.)),\n",
              " (tensor([0.0133, 0.9986]), tensor(0.)),\n",
              " (tensor([2.4016, 0.0406]), tensor(0.)),\n",
              " (tensor([ 3.4008, -0.4155]), tensor(0.)),\n",
              " (tensor([0.5022, 0.9083]), tensor(0.)),\n",
              " (tensor([ 2.5602, -0.0415]), tensor(0.)),\n",
              " (tensor([ 4.7010, -0.6828]), tensor(0.)),\n",
              " (tensor([ 5.6048, -0.6724]), tensor(0.)),\n",
              " (tensor([0.0397, 0.9958]), tensor(0.)),\n",
              " (tensor([ 6.2072, -0.6329]), tensor(0.)),\n",
              " (tensor([ 2.5834, -0.0533]), tensor(0.)),\n",
              " (tensor([ 4.8945, -0.6904]), tensor(0.)),\n",
              " (tensor([0.6499, 0.8661]), tensor(0.)),\n",
              " (tensor([ 5.8609, -0.6560]), tensor(0.)),\n",
              " (tensor([ 5.3609, -0.6848]), tensor(0.)),\n",
              " (tensor([2.1776, 0.1595]), tensor(0.)),\n",
              " (tensor([1.1286, 0.6880]), tensor(0.)),\n",
              " (tensor([ 2.7857, -0.1535]), tensor(0.)),\n",
              " (tensor([2.4597, 0.0103]), tensor(0.)),\n",
              " (tensor([1.9495, 0.2816]), tensor(0.)),\n",
              " (tensor([ 5.2161, -0.6895]), tensor(0.)),\n",
              " (tensor([0.9587, 0.7578]), tensor(0.)),\n",
              " (tensor([1.7526, 0.3859]), tensor(0.)),\n",
              " (tensor([0.5210, 0.9033]), tensor(0.)),\n",
              " (tensor([1.4915, 0.5192]), tensor(0.)),\n",
              " (tensor([ 6.0917, -0.6404]), tensor(0.)),\n",
              " (tensor([ 4.6913, -0.6822]), tensor(0.)),\n",
              " (tensor([ 4.2651, -0.6379]), tensor(0.)),\n",
              " (tensor([ 5.9093, -0.6527]), tensor(0.)),\n",
              " (tensor([2.1385, 0.1804]), tensor(0.)),\n",
              " (tensor([ 3.6075, -0.4853]), tensor(0.)),\n",
              " (tensor([0.1405, 0.9827]), tensor(0.)),\n",
              " (tensor([1.3852, 0.5710]), tensor(0.)),\n",
              " (tensor([ 6.1304, -0.6379]), tensor(0.)),\n",
              " (tensor([ 3.6950, -0.5117]), tensor(0.)),\n",
              " (tensor([ 4.9168, -0.6908]), tensor(0.)),\n",
              " (tensor([ 5.6938, -0.6670]), tensor(0.)),\n",
              " (tensor([ 3.4441, -0.4309]), tensor(0.)),\n",
              " (tensor([ 3.6546, -0.4998]), tensor(0.)),\n",
              " (tensor([ 5.4885, -0.6789]), tensor(0.)),\n",
              " (tensor([1.9301, 0.2920]), tensor(0.)),\n",
              " (tensor([ 2.6562, -0.0900]), tensor(0.)),\n",
              " (tensor([2.1557, 0.1712]), tensor(0.)),\n",
              " (tensor([ 3.6075, -0.4853]), tensor(0.)),\n",
              " (tensor([0.2894, 0.9572]), tensor(0.)),\n",
              " (tensor([ 3.9654, -0.5816]), tensor(0.)),\n",
              " (tensor([ 3.1665, -0.3245]), tensor(0.)),\n",
              " (tensor([ 4.3416, -0.6488]), tensor(0.)),\n",
              " (tensor([ 4.5592, -0.6727]), tensor(0.)),\n",
              " (tensor([1.8469, 0.3362]), tensor(0.)),\n",
              " (tensor([ 5.2813, -0.6877]), tensor(0.)),\n",
              " (tensor([1.6090, 0.4602]), tensor(0.)),\n",
              " (tensor([ 4.1439, -0.6178]), tensor(0.)),\n",
              " (tensor([ 5.3397, -0.6856]), tensor(0.)),\n",
              " (tensor([ 5.5977, -0.6729]), tensor(0.)),\n",
              " (tensor([0.9867, 0.7468]), tensor(0.)),\n",
              " (tensor([0.1110, 0.9868]), tensor(0.)),\n",
              " (tensor([0.7904, 0.8200]), tensor(0.)),\n",
              " (tensor([1.7885, 0.3671]), tensor(0.)),\n",
              " (tensor([2.4435, 0.0187]), tensor(0.)),\n",
              " (tensor([ 5.1152, -0.6914]), tensor(0.)),\n",
              " (tensor([1.4333, 0.5478]), tensor(0.)),\n",
              " (tensor([ 5.1207, -0.6913]), tensor(0.)),\n",
              " (tensor([ 5.2081, -0.6897]), tensor(0.)),\n",
              " (tensor([ 3.7199, -0.5189]), tensor(0.)),\n",
              " (tensor([1.2309, 0.6428]), tensor(0.)),\n",
              " (tensor([ 2.9429, -0.2272]), tensor(0.)),\n",
              " (tensor([ 4.0850, -0.6067]), tensor(0.)),\n",
              " (tensor([0.5435, 0.8971]), tensor(0.)),\n",
              " (tensor([0.8892, 0.7844]), tensor(0.)),\n",
              " (tensor([1.4463, 0.5414]), tensor(0.)),\n",
              " (tensor([ 5.8560, -0.6564]), tensor(0.)),\n",
              " (tensor([ 3.5588, -0.4698]), tensor(0.)),\n",
              " (tensor([ 2.7349, -0.1288]), tensor(0.)),\n",
              " (tensor([1.9662, 0.2727]), tensor(0.)),\n",
              " (tensor([ 3.4312, -0.4263]), tensor(0.)),\n",
              " (tensor([0.6458, 0.8674]), tensor(0.)),\n",
              " (tensor([1.2312, 0.6427]), tensor(0.)),\n",
              " (tensor([1.9285, 0.2929]), tensor(0.)),\n",
              " (tensor([ 3.9575, -0.5798]), tensor(0.)),\n",
              " (tensor([1.4519, 0.5387]), tensor(0.)),\n",
              " (tensor([ 5.0466, -0.6918]), tensor(0.)),\n",
              " (tensor([ 5.5666, -0.6747]), tensor(0.)),\n",
              " (tensor([ 3.3753, -0.4061]), tensor(0.)),\n",
              " (tensor([ 3.3601, -0.4005]), tensor(0.)),\n",
              " (tensor([1.1131, 0.6946]), tensor(0.)),\n",
              " (tensor([ 3.0817, -0.2888]), tensor(0.)),\n",
              " (tensor([ 5.4940, -0.6786]), tensor(0.)),\n",
              " (tensor([ 4.0011, -0.5894]), tensor(0.)),\n",
              " (tensor([0.4746, 0.9154]), tensor(0.)),\n",
              " (tensor([ 5.3596, -0.6848]), tensor(0.)),\n",
              " (tensor([ 2.7475, -0.1350]), tensor(0.)),\n",
              " (tensor([1.9077, 0.3040]), tensor(0.)),\n",
              " (tensor([ 6.0425, -0.6437]), tensor(0.)),\n",
              " (tensor([ 4.9616, -0.6915]), tensor(0.)),\n",
              " (tensor([1.1067, 0.6973]), tensor(0.)),\n",
              " (tensor([ 5.3550, -0.6850]), tensor(0.)),\n",
              " (tensor([ 5.1652, -0.6906]), tensor(0.)),\n",
              " (tensor([ 3.1502, -0.3178]), tensor(0.)),\n",
              " (tensor([ 5.9370, -0.6509]), tensor(0.)),\n",
              " (tensor([ 4.3200, -0.6459]), tensor(0.)),\n",
              " (tensor([ 4.0242, -0.5944]), tensor(0.)),\n",
              " (tensor([2.0998, 0.2011]), tensor(0.)),\n",
              " (tensor([ 5.3336, -0.6858]), tensor(0.)),\n",
              " (tensor([0.9443, 0.7634]), tensor(0.)),\n",
              " (tensor([ 3.3482, -0.3961]), tensor(0.)),\n",
              " (tensor([2.2798, 0.1050]), tensor(0.)),\n",
              " (tensor([ 2.5580, -0.0404]), tensor(0.)),\n",
              " (tensor([ 5.9957, -0.6469]), tensor(0.)),\n",
              " (tensor([1.7527, 0.3859]), tensor(0.)),\n",
              " (tensor([ 2.5641, -0.0435]), tensor(0.)),\n",
              " (tensor([ 3.5882, -0.4792]), tensor(0.)),\n",
              " (tensor([0.6094, 0.8783]), tensor(0.)),\n",
              " (tensor([ 3.3389, -0.3926]), tensor(0.)),\n",
              " (tensor([ 5.3663, -0.6845]), tensor(0.)),\n",
              " (tensor([2.3731, 0.0556]), tensor(0.)),\n",
              " (tensor([0.1036, 0.9878]), tensor(0.)),\n",
              " (tensor([ 5.3596, -0.6848]), tensor(0.)),\n",
              " (tensor([ 3.7522, -0.5280]), tensor(0.)),\n",
              " (tensor([1.8071, 0.3573]), tensor(0.)),\n",
              " (tensor([1.5979, 0.4658]), tensor(0.)),\n",
              " (tensor([ 4.1155, -0.6125]), tensor(0.)),\n",
              " (tensor([2.2665, 0.1120]), tensor(0.)),\n",
              " (tensor([0.6651, 0.8614]), tensor(0.)),\n",
              " (tensor([ 5.1730, -0.6905]), tensor(0.)),\n",
              " (tensor([ 2.7134, -0.1183]), tensor(0.)),\n",
              " (tensor([ 5.9170, -0.6522]), tensor(0.)),\n",
              " (tensor([ 5.1153, -0.6914]), tensor(0.)),\n",
              " (tensor([2.2260, 0.1336]), tensor(0.)),\n",
              " (tensor([0.3402, 0.9468]), tensor(0.)),\n",
              " (tensor([ 3.0011, -0.2534]), tensor(0.)),\n",
              " (tensor([ 5.3616, -0.6847]), tensor(0.)),\n",
              " (tensor([ 5.6379, -0.6705]), tensor(0.)),\n",
              " (tensor([1.2099, 0.6523]), tensor(0.)),\n",
              " (tensor([ 3.5753, -0.4751]), tensor(0.)),\n",
              " (tensor([0.6806, 0.8565]), tensor(0.)),\n",
              " (tensor([ 2.9317, -0.2221]), tensor(0.)),\n",
              " (tensor([ 6.1062, -0.6395]), tensor(0.)),\n",
              " (tensor([2.1695, 0.1638]), tensor(0.)),\n",
              " (tensor([0.4114, 0.9309]), tensor(0.)),\n",
              " (tensor([1.3365, 0.5941]), tensor(0.)),\n",
              " (tensor([0.7859, 0.8216]), tensor(0.)),\n",
              " (tensor([ 3.8891, -0.5637]), tensor(0.)),\n",
              " (tensor([ 4.8062, -0.6878]), tensor(0.)),\n",
              " (tensor([1.6866, 0.4203]), tensor(0.)),\n",
              " (tensor([ 5.8264, -0.6583]), tensor(0.)),\n",
              " (tensor([ 5.9445, -0.6503]), tensor(0.)),\n",
              " (tensor([1.6125, 0.4583]), tensor(0.)),\n",
              " (tensor([ 3.5753, -0.4751]), tensor(0.)),\n",
              " (tensor([0.8906, 0.7839]), tensor(0.)),\n",
              " (tensor([1.2157, 0.6497]), tensor(0.)),\n",
              " (tensor([1.9109, 0.3022]), tensor(0.)),\n",
              " (tensor([ 2.5916, -0.0575]), tensor(0.)),\n",
              " (tensor([ 4.0733, -0.6044]), tensor(0.)),\n",
              " (tensor([ 4.2052, -0.6284]), tensor(0.)),\n",
              " (tensor([ 3.2594, -0.3620]), tensor(0.)),\n",
              " (tensor([ 5.9165, -0.6523]), tensor(0.)),\n",
              " (tensor([ 5.4344, -0.6815]), tensor(0.)),\n",
              " (tensor([2.3857, 0.0489]), tensor(0.)),\n",
              " (tensor([2.3773, 0.0534]), tensor(0.)),\n",
              " (tensor([1.5501, 0.4900]), tensor(0.)),\n",
              " (tensor([2.0820, 0.2107]), tensor(0.)),\n",
              " (tensor([2.4673, 0.0063]), tensor(0.)),\n",
              " (tensor([ 2.8645, -0.1909]), tensor(0.)),\n",
              " (tensor([0.0728, 0.9918]), tensor(0.)),\n",
              " (tensor([ 3.6072, -0.4852]), tensor(0.)),\n",
              " (tensor([ 3.2294, -0.3501]), tensor(0.)),\n",
              " (tensor([2.4208, 0.0306]), tensor(0.)),\n",
              " (tensor([ 5.1009, -0.6915]), tensor(0.)),\n",
              " (tensor([2.4371, 0.0220]), tensor(0.)),\n",
              " (tensor([ 4.1762, -0.6235]), tensor(0.)),\n",
              " (tensor([ 4.5850, -0.6748]), tensor(0.)),\n",
              " (tensor([ 3.7800, -0.5356]), tensor(0.)),\n",
              " (tensor([ 3.5412, -0.4640]), tensor(0.)),\n",
              " (tensor([2.0630, 0.2209]), tensor(0.)),\n",
              " (tensor([ 3.3692, -0.4039]), tensor(0.)),\n",
              " (tensor([1.7256, 0.4000]), tensor(0.)),\n",
              " (tensor([ 3.1446, -0.3154]), tensor(0.)),\n",
              " (tensor([0.7701, 0.8270]), tensor(0.)),\n",
              " (tensor([ 4.4652, -0.6637]), tensor(0.)),\n",
              " (tensor([1.2019, 0.6558]), tensor(0.)),\n",
              " (tensor([ 3.4920, -0.4475]), tensor(0.)),\n",
              " (tensor([1.4969, 0.5165]), tensor(0.)),\n",
              " (tensor([1.5324, 0.4989]), tensor(0.)),\n",
              " (tensor([0.4772, 0.9148]), tensor(0.)),\n",
              " (tensor([ 3.8523, -0.5546]), tensor(0.)),\n",
              " (tensor([0.1668, 0.9787]), tensor(0.)),\n",
              " (tensor([ 2.5397, -0.0310]), tensor(0.)),\n",
              " (tensor([ 2.8618, -0.1897]), tensor(0.)),\n",
              " (tensor([ 2.9506, -0.2307]), tensor(0.)),\n",
              " (tensor([ 3.5843, -0.4779]), tensor(0.)),\n",
              " (tensor([1.3400, 0.5925]), tensor(0.)),\n",
              " (tensor([ 4.7542, -0.6856]), tensor(0.)),\n",
              " (tensor([ 4.7226, -0.6840]), tensor(0.)),\n",
              " (tensor([ 4.7120, -0.6834]), tensor(0.)),\n",
              " (tensor([0.0500, 0.9946]), tensor(0.)),\n",
              " (tensor([ 4.8842, -0.6901]), tensor(0.)),\n",
              " (tensor([ 5.7721, -0.6620]), tensor(0.)),\n",
              " (tensor([1.0514, 0.7206]), tensor(0.)),\n",
              " (tensor([1.6566, 0.4358]), tensor(0.)),\n",
              " (tensor([1.3532, 0.5862]), tensor(0.)),\n",
              " (tensor([0.2861, 0.9578]), tensor(0.)),\n",
              " (tensor([2.2595, 0.1157]), tensor(0.)),\n",
              " (tensor([ 5.3497, -0.6852]), tensor(0.)),\n",
              " (tensor([1.9796, 0.2655]), tensor(0.)),\n",
              " (tensor([0.0732, 0.9918]), tensor(0.)),\n",
              " (tensor([ 5.8596, -0.6561]), tensor(0.)),\n",
              " (tensor([0.8481, 0.7996]), tensor(0.)),\n",
              " (tensor([2.2621, 0.1144]), tensor(0.)),\n",
              " (tensor([0.5999, 0.8811]), tensor(0.)),\n",
              " (tensor([0.8721, 0.7908]), tensor(0.)),\n",
              " (tensor([0.1630, 0.9793]), tensor(0.)),\n",
              " (tensor([ 3.9670, -0.5819]), tensor(0.)),\n",
              " (tensor([2.0562, 0.2245]), tensor(0.)),\n",
              " (tensor([ 4.0282, -0.5952]), tensor(0.)),\n",
              " (tensor([ 2.9094, -0.2118]), tensor(0.)),\n",
              " (tensor([1.5042, 0.5129]), tensor(0.)),\n",
              " (tensor([1.1867, 0.6626]), tensor(0.)),\n",
              " (tensor([0.0698, 0.9922]), tensor(0.)),\n",
              " (tensor([ 3.2478, -0.3574]), tensor(0.)),\n",
              " (tensor([2.1591, 0.1694]), tensor(0.)),\n",
              " (tensor([ 3.8270, -0.5481]), tensor(0.)),\n",
              " (tensor([ 3.0551, -0.2772]), tensor(0.)),\n",
              " (tensor([ 3.4831, -0.4445]), tensor(0.)),\n",
              " (tensor([ 6.1194, -0.6386]), tensor(0.)),\n",
              " (tensor([ 4.7396, -0.6849]), tensor(0.)),\n",
              " (tensor([0.0491, 0.9947]), tensor(0.)),\n",
              " (tensor([0.8625, 0.7943]), tensor(0.)),\n",
              " (tensor([ 3.3296, -0.3891]), tensor(0.)),\n",
              " (tensor([ 4.7366, -0.6847]), tensor(0.)),\n",
              " (tensor([ 3.6613, -0.5018]), tensor(0.)),\n",
              " (tensor([ 3.7299, -0.5218]), tensor(0.)),\n",
              " (tensor([ 4.4946, -0.6667]), tensor(0.)),\n",
              " (tensor([ 4.9974, -0.6918]), tensor(0.)),\n",
              " (tensor([ 4.5164, -0.6688]), tensor(0.)),\n",
              " (tensor([0.0632, 0.9930]), tensor(0.)),\n",
              " (tensor([ 4.3621, -0.6515]), tensor(0.)),\n",
              " (tensor([1.6919, 0.4175]), tensor(0.)),\n",
              " (tensor([ 2.6725, -0.0981]), tensor(0.)),\n",
              " (tensor([0.4218, 0.9284]), tensor(0.)),\n",
              " (tensor([ 5.0440, -0.6919]), tensor(0.)),\n",
              " (tensor([1.0090, 0.7378]), tensor(0.)),\n",
              " (tensor([1.7603, 0.3819]), tensor(0.)),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_data[:, 0], train_data[:, 1], \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xfByOVLdzuQU",
        "outputId": "82087b44-e461-4c83-93f1-e78d0be23816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd14bf1ff50>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UmuKkjCr2Hsr",
        "outputId": "69590c55-d40f-4cc1-d1c3-f6d55a47b9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "4gtYEWou2Hmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "aiLIJuid2VrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "KL5QXEa72WkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200  # количество эпох\n",
        "lr = 0.0002  # шаг обучения\n",
        "\n",
        "b1 = 0.5  # гиперпараметр для оптимайзера Adam\n",
        "b2 = 0.999  # гиперпараметр для оптимайзера Adam\n",
        "\n",
        "latent_dim = 100  # Размерность случайного вектора, который подается на вход генератору\n",
        "\n",
        "sample_interval = 500  # количество итераций для отображения процесса обучения"
      ],
      "metadata": {
        "id": "tQhYxeD52w8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Для каждой нейронки свой опитимизатор\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    generator.parameters(),\n",
        "    lr=lr, \n",
        "    betas=(b1, b2)\n",
        ")\n",
        "optimizer_D = torch.optim.Adam(\n",
        "    discriminator.parameters(), \n",
        "    lr=lr, \n",
        "    betas=(b1, b2)\n",
        ")\n",
        "\n",
        "# Но вот функция ошибки у нас будет одна общая\n",
        "adversarial_loss = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "Xn-BAxjf2zRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_loss_history = []\n",
        "g_loss_history = []"
      ],
      "metadata": {
        "id": "CuxoVc1T25Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for n, (real_samples, _) in enumerate(train_loader):\n",
        "        # Данные для обучения дискриминатора\n",
        "        real_samples_labels = torch.ones((batch_size, 1))\n",
        "        latent_space_samples = torch.randn((batch_size, 2))\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
        "        all_samples = torch.cat((real_samples, generated_samples))\n",
        "        all_samples_labels = torch.cat(\n",
        "            (real_samples_labels, generated_samples_labels)\n",
        "        )\n",
        "        # Обучение дискриминатора\n",
        "        discriminator.zero_grad()\n",
        "        output_discriminator = discriminator(all_samples)\n",
        "        loss_discriminator = adversarial_loss(\n",
        "            output_discriminator, all_samples_labels)\n",
        "        loss_discriminator.backward()\n",
        "        optimizer_D.step()\n",
        "        # Данные для обучения генератора\n",
        "        latent_space_samples = torch.randn((batch_size, 2))\n",
        "        # Обучение генератора\n",
        "        generator.zero_grad()\n",
        "        generated_samples = generator(latent_space_samples)\n",
        "        output_discriminator_generated = discriminator(generated_samples)\n",
        "        loss_generator = adversarial_loss(\n",
        "            output_discriminator_generated, real_samples_labels\n",
        "        )\n",
        "        loss_generator.backward()\n",
        "        optimizer_G.step()\n",
        "        # Показать ошибку\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
        "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G82HOA9W26pM",
        "outputId": "6b610d56-4794-495c-b921-c0f5dd93860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss D.: 0.6504845023155212\n",
            "Epoch: 0 Loss G.: 0.6721874475479126\n",
            "Epoch: 0 Loss D.: 0.6436079740524292\n",
            "Epoch: 0 Loss G.: 0.6790587902069092\n",
            "Epoch: 0 Loss D.: 0.6358855962753296\n",
            "Epoch: 0 Loss G.: 0.6693841218948364\n",
            "Epoch: 0 Loss D.: 0.6414862275123596\n",
            "Epoch: 0 Loss G.: 0.6755834221839905\n",
            "Epoch: 0 Loss D.: 0.6246877908706665\n",
            "Epoch: 0 Loss G.: 0.6822267770767212\n",
            "Epoch: 0 Loss D.: 0.6101246476173401\n",
            "Epoch: 0 Loss G.: 0.680322527885437\n",
            "Epoch: 0 Loss D.: 0.5941847562789917\n",
            "Epoch: 0 Loss G.: 0.6784390211105347\n",
            "Epoch: 0 Loss D.: 0.5836411714553833\n",
            "Epoch: 0 Loss G.: 0.6824772357940674\n",
            "Epoch: 0 Loss D.: 0.5999984741210938\n",
            "Epoch: 0 Loss G.: 0.669795572757721\n",
            "Epoch: 0 Loss D.: 0.6044448614120483\n",
            "Epoch: 0 Loss G.: 0.6831931471824646\n",
            "Epoch: 0 Loss D.: 0.5558983087539673\n",
            "Epoch: 0 Loss G.: 0.680211067199707\n",
            "Epoch: 0 Loss D.: 0.5766326189041138\n",
            "Epoch: 0 Loss G.: 0.6826887726783752\n",
            "Epoch: 0 Loss D.: 0.5462386608123779\n",
            "Epoch: 0 Loss G.: 0.6876553893089294\n",
            "Epoch: 0 Loss D.: 0.5509759783744812\n",
            "Epoch: 0 Loss G.: 0.6859118938446045\n",
            "Epoch: 0 Loss D.: 0.5572104454040527\n",
            "Epoch: 0 Loss G.: 0.6835678219795227\n",
            "Epoch: 0 Loss D.: 0.5313823223114014\n",
            "Epoch: 0 Loss G.: 0.6911920309066772\n",
            "Epoch: 0 Loss D.: 0.5236461162567139\n",
            "Epoch: 0 Loss G.: 0.6950730681419373\n",
            "Epoch: 0 Loss D.: 0.5125468969345093\n",
            "Epoch: 0 Loss G.: 0.6912340521812439\n",
            "Epoch: 0 Loss D.: 0.5261185765266418\n",
            "Epoch: 0 Loss G.: 0.7005971074104309\n",
            "Epoch: 0 Loss D.: 0.5187680721282959\n",
            "Epoch: 0 Loss G.: 0.7006504535675049\n",
            "Epoch: 0 Loss D.: 0.49078378081321716\n",
            "Epoch: 0 Loss G.: 0.6972447037696838\n",
            "Epoch: 0 Loss D.: 0.4750431478023529\n",
            "Epoch: 0 Loss G.: 0.7022945284843445\n",
            "Epoch: 0 Loss D.: 0.4778746962547302\n",
            "Epoch: 0 Loss G.: 0.7050197124481201\n",
            "Epoch: 0 Loss D.: 0.4599212408065796\n",
            "Epoch: 0 Loss G.: 0.6932231783866882\n",
            "Epoch: 0 Loss D.: 0.4885682463645935\n",
            "Epoch: 0 Loss G.: 0.7087525129318237\n",
            "Epoch: 0 Loss D.: 0.469081312417984\n",
            "Epoch: 0 Loss G.: 0.7202404737472534\n",
            "Epoch: 0 Loss D.: 0.4826295077800751\n",
            "Epoch: 0 Loss G.: 0.7338681817054749\n",
            "Epoch: 0 Loss D.: 0.4920690059661865\n",
            "Epoch: 0 Loss G.: 0.7221996188163757\n",
            "Epoch: 0 Loss D.: 0.45481231808662415\n",
            "Epoch: 0 Loss G.: 0.7286134362220764\n",
            "Epoch: 0 Loss D.: 0.4878799319267273\n",
            "Epoch: 0 Loss G.: 0.736677348613739\n",
            "Epoch: 0 Loss D.: 0.4351668357849121\n",
            "Epoch: 0 Loss G.: 0.7468673586845398\n",
            "Epoch: 0 Loss D.: 0.4440838098526001\n",
            "Epoch: 0 Loss G.: 0.7448223829269409\n",
            "Epoch: 10 Loss D.: 0.5710552930831909\n",
            "Epoch: 10 Loss G.: 1.1686140298843384\n",
            "Epoch: 10 Loss D.: 0.5648965239524841\n",
            "Epoch: 10 Loss G.: 1.0683258771896362\n",
            "Epoch: 10 Loss D.: 0.6754549145698547\n",
            "Epoch: 10 Loss G.: 1.0663657188415527\n",
            "Epoch: 10 Loss D.: 0.49162572622299194\n",
            "Epoch: 10 Loss G.: 0.977016806602478\n",
            "Epoch: 10 Loss D.: 0.4518757164478302\n",
            "Epoch: 10 Loss G.: 1.0008394718170166\n",
            "Epoch: 10 Loss D.: 0.5585779547691345\n",
            "Epoch: 10 Loss G.: 1.1244293451309204\n",
            "Epoch: 10 Loss D.: 0.4918709099292755\n",
            "Epoch: 10 Loss G.: 1.0590794086456299\n",
            "Epoch: 10 Loss D.: 0.5046626329421997\n",
            "Epoch: 10 Loss G.: 1.1340727806091309\n",
            "Epoch: 10 Loss D.: 0.6069916486740112\n",
            "Epoch: 10 Loss G.: 0.9963952302932739\n",
            "Epoch: 10 Loss D.: 0.4953930377960205\n",
            "Epoch: 10 Loss G.: 1.2103427648544312\n",
            "Epoch: 10 Loss D.: 0.6040725111961365\n",
            "Epoch: 10 Loss G.: 1.077004313468933\n",
            "Epoch: 10 Loss D.: 0.6047071218490601\n",
            "Epoch: 10 Loss G.: 1.0957555770874023\n",
            "Epoch: 10 Loss D.: 0.5705368518829346\n",
            "Epoch: 10 Loss G.: 1.0158241987228394\n",
            "Epoch: 10 Loss D.: 0.5536109805107117\n",
            "Epoch: 10 Loss G.: 1.059449553489685\n",
            "Epoch: 10 Loss D.: 0.6300784349441528\n",
            "Epoch: 10 Loss G.: 0.8896883130073547\n",
            "Epoch: 10 Loss D.: 0.6571693420410156\n",
            "Epoch: 10 Loss G.: 0.9210295677185059\n",
            "Epoch: 10 Loss D.: 0.527999997138977\n",
            "Epoch: 10 Loss G.: 0.9710836410522461\n",
            "Epoch: 10 Loss D.: 0.5273289680480957\n",
            "Epoch: 10 Loss G.: 1.032493233680725\n",
            "Epoch: 10 Loss D.: 0.4972829818725586\n",
            "Epoch: 10 Loss G.: 1.032493233680725\n",
            "Epoch: 10 Loss D.: 0.5435630083084106\n",
            "Epoch: 10 Loss G.: 0.9912984371185303\n",
            "Epoch: 10 Loss D.: 0.5171688795089722\n",
            "Epoch: 10 Loss G.: 1.028240442276001\n",
            "Epoch: 10 Loss D.: 0.503368616104126\n",
            "Epoch: 10 Loss G.: 1.0384259223937988\n",
            "Epoch: 10 Loss D.: 0.5254599452018738\n",
            "Epoch: 10 Loss G.: 1.0436127185821533\n",
            "Epoch: 10 Loss D.: 0.5444132089614868\n",
            "Epoch: 10 Loss G.: 1.0033255815505981\n",
            "Epoch: 10 Loss D.: 0.5638478398323059\n",
            "Epoch: 10 Loss G.: 1.0594772100448608\n",
            "Epoch: 10 Loss D.: 0.5988603830337524\n",
            "Epoch: 10 Loss G.: 1.0128782987594604\n",
            "Epoch: 10 Loss D.: 0.5629905462265015\n",
            "Epoch: 10 Loss G.: 1.0068893432617188\n",
            "Epoch: 10 Loss D.: 0.46961861848831177\n",
            "Epoch: 10 Loss G.: 1.0773353576660156\n",
            "Epoch: 10 Loss D.: 0.47372013330459595\n",
            "Epoch: 10 Loss G.: 1.04637610912323\n",
            "Epoch: 10 Loss D.: 0.6075876355171204\n",
            "Epoch: 10 Loss G.: 1.0830429792404175\n",
            "Epoch: 10 Loss D.: 0.5933061838150024\n",
            "Epoch: 10 Loss G.: 0.9131935834884644\n",
            "Epoch: 10 Loss D.: 0.5557589530944824\n",
            "Epoch: 10 Loss G.: 1.0454626083374023\n",
            "Epoch: 20 Loss D.: 0.6763139367103577\n",
            "Epoch: 20 Loss G.: 0.830880880355835\n",
            "Epoch: 20 Loss D.: 0.6787298917770386\n",
            "Epoch: 20 Loss G.: 0.7756046056747437\n",
            "Epoch: 20 Loss D.: 0.6587958335876465\n",
            "Epoch: 20 Loss G.: 0.7603363990783691\n",
            "Epoch: 20 Loss D.: 0.6669906973838806\n",
            "Epoch: 20 Loss G.: 0.7499467134475708\n",
            "Epoch: 20 Loss D.: 0.6743280291557312\n",
            "Epoch: 20 Loss G.: 0.7923693656921387\n",
            "Epoch: 20 Loss D.: 0.7039111256599426\n",
            "Epoch: 20 Loss G.: 0.8733701705932617\n",
            "Epoch: 20 Loss D.: 0.6408887505531311\n",
            "Epoch: 20 Loss G.: 0.7977423667907715\n",
            "Epoch: 20 Loss D.: 0.6637603044509888\n",
            "Epoch: 20 Loss G.: 0.7639788389205933\n",
            "Epoch: 20 Loss D.: 0.6623565554618835\n",
            "Epoch: 20 Loss G.: 0.7869186401367188\n",
            "Epoch: 20 Loss D.: 0.6488651633262634\n",
            "Epoch: 20 Loss G.: 0.8069548606872559\n",
            "Epoch: 20 Loss D.: 0.687934935092926\n",
            "Epoch: 20 Loss G.: 0.8038042187690735\n",
            "Epoch: 20 Loss D.: 0.6831900477409363\n",
            "Epoch: 20 Loss G.: 0.8571547865867615\n",
            "Epoch: 20 Loss D.: 0.6717312932014465\n",
            "Epoch: 20 Loss G.: 0.7170414328575134\n",
            "Epoch: 20 Loss D.: 0.6696426868438721\n",
            "Epoch: 20 Loss G.: 0.8764053583145142\n",
            "Epoch: 20 Loss D.: 0.6556957364082336\n",
            "Epoch: 20 Loss G.: 0.7922104597091675\n",
            "Epoch: 20 Loss D.: 0.6450852155685425\n",
            "Epoch: 20 Loss G.: 0.7534869909286499\n",
            "Epoch: 20 Loss D.: 0.6717888712882996\n",
            "Epoch: 20 Loss G.: 0.8291828632354736\n",
            "Epoch: 20 Loss D.: 0.6855748891830444\n",
            "Epoch: 20 Loss G.: 0.8057966828346252\n",
            "Epoch: 20 Loss D.: 0.6983444690704346\n",
            "Epoch: 20 Loss G.: 0.7952170968055725\n",
            "Epoch: 20 Loss D.: 0.6961292624473572\n",
            "Epoch: 20 Loss G.: 0.8065257668495178\n",
            "Epoch: 20 Loss D.: 0.6525200009346008\n",
            "Epoch: 20 Loss G.: 0.8184487819671631\n",
            "Epoch: 20 Loss D.: 0.6856975555419922\n",
            "Epoch: 20 Loss G.: 0.7117905616760254\n",
            "Epoch: 20 Loss D.: 0.6744621396064758\n",
            "Epoch: 20 Loss G.: 0.8017377853393555\n",
            "Epoch: 20 Loss D.: 0.6455932259559631\n",
            "Epoch: 20 Loss G.: 0.7341957092285156\n",
            "Epoch: 20 Loss D.: 0.6805434226989746\n",
            "Epoch: 20 Loss G.: 0.800015926361084\n",
            "Epoch: 20 Loss D.: 0.6676602363586426\n",
            "Epoch: 20 Loss G.: 0.7931632399559021\n",
            "Epoch: 20 Loss D.: 0.6838549971580505\n",
            "Epoch: 20 Loss G.: 0.7646756172180176\n",
            "Epoch: 20 Loss D.: 0.695914089679718\n",
            "Epoch: 20 Loss G.: 0.763552188873291\n",
            "Epoch: 20 Loss D.: 0.6478602886199951\n",
            "Epoch: 20 Loss G.: 0.7747833132743835\n",
            "Epoch: 20 Loss D.: 0.6706782579421997\n",
            "Epoch: 20 Loss G.: 0.8417880535125732\n",
            "Epoch: 20 Loss D.: 0.6417495608329773\n",
            "Epoch: 20 Loss G.: 0.8200615644454956\n",
            "Epoch: 20 Loss D.: 0.6542900204658508\n",
            "Epoch: 20 Loss G.: 0.7577818632125854\n",
            "Epoch: 30 Loss D.: 0.683738112449646\n",
            "Epoch: 30 Loss G.: 0.7086978554725647\n",
            "Epoch: 30 Loss D.: 0.685793399810791\n",
            "Epoch: 30 Loss G.: 0.7055148482322693\n",
            "Epoch: 30 Loss D.: 0.6911819577217102\n",
            "Epoch: 30 Loss G.: 0.7340295314788818\n",
            "Epoch: 30 Loss D.: 0.6700785160064697\n",
            "Epoch: 30 Loss G.: 0.7270209193229675\n",
            "Epoch: 30 Loss D.: 0.6837506294250488\n",
            "Epoch: 30 Loss G.: 0.7632130980491638\n",
            "Epoch: 30 Loss D.: 0.6994909644126892\n",
            "Epoch: 30 Loss G.: 0.7073426246643066\n",
            "Epoch: 30 Loss D.: 0.6815957427024841\n",
            "Epoch: 30 Loss G.: 0.7663266062736511\n",
            "Epoch: 30 Loss D.: 0.6910874247550964\n",
            "Epoch: 30 Loss G.: 0.7061012387275696\n",
            "Epoch: 30 Loss D.: 0.6884795427322388\n",
            "Epoch: 30 Loss G.: 0.718134880065918\n",
            "Epoch: 30 Loss D.: 0.6904184818267822\n",
            "Epoch: 30 Loss G.: 0.7261868119239807\n",
            "Epoch: 30 Loss D.: 0.6760343313217163\n",
            "Epoch: 30 Loss G.: 0.7653518915176392\n",
            "Epoch: 30 Loss D.: 0.6778473854064941\n",
            "Epoch: 30 Loss G.: 0.6976422071456909\n",
            "Epoch: 30 Loss D.: 0.6953966021537781\n",
            "Epoch: 30 Loss G.: 0.7516754269599915\n",
            "Epoch: 30 Loss D.: 0.6943701505661011\n",
            "Epoch: 30 Loss G.: 0.735428512096405\n",
            "Epoch: 30 Loss D.: 0.6907951235771179\n",
            "Epoch: 30 Loss G.: 0.7409870624542236\n",
            "Epoch: 30 Loss D.: 0.6786212921142578\n",
            "Epoch: 30 Loss G.: 0.7531786561012268\n",
            "Epoch: 30 Loss D.: 0.6672083735466003\n",
            "Epoch: 30 Loss G.: 0.7417793273925781\n",
            "Epoch: 30 Loss D.: 0.6994024515151978\n",
            "Epoch: 30 Loss G.: 0.7020358443260193\n",
            "Epoch: 30 Loss D.: 0.6812989711761475\n",
            "Epoch: 30 Loss G.: 0.7105597853660583\n",
            "Epoch: 30 Loss D.: 0.6706410646438599\n",
            "Epoch: 30 Loss G.: 0.7155075669288635\n",
            "Epoch: 30 Loss D.: 0.6970667243003845\n",
            "Epoch: 30 Loss G.: 0.728713870048523\n",
            "Epoch: 30 Loss D.: 0.6898025870323181\n",
            "Epoch: 30 Loss G.: 0.7018652558326721\n",
            "Epoch: 30 Loss D.: 0.6848487257957458\n",
            "Epoch: 30 Loss G.: 0.7333699464797974\n",
            "Epoch: 30 Loss D.: 0.6961292028427124\n",
            "Epoch: 30 Loss G.: 0.7292212843894958\n",
            "Epoch: 30 Loss D.: 0.6749199628829956\n",
            "Epoch: 30 Loss G.: 0.7080861926078796\n",
            "Epoch: 30 Loss D.: 0.6888310313224792\n",
            "Epoch: 30 Loss G.: 0.7125471830368042\n",
            "Epoch: 30 Loss D.: 0.689617931842804\n",
            "Epoch: 30 Loss G.: 0.6937106847763062\n",
            "Epoch: 30 Loss D.: 0.69914710521698\n",
            "Epoch: 30 Loss G.: 0.7445868253707886\n",
            "Epoch: 30 Loss D.: 0.6808357238769531\n",
            "Epoch: 30 Loss G.: 0.7195712327957153\n",
            "Epoch: 30 Loss D.: 0.684272825717926\n",
            "Epoch: 30 Loss G.: 0.7240910530090332\n",
            "Epoch: 30 Loss D.: 0.6834605932235718\n",
            "Epoch: 30 Loss G.: 0.7277466654777527\n",
            "Epoch: 30 Loss D.: 0.7058922052383423\n",
            "Epoch: 30 Loss G.: 0.7195589542388916\n",
            "Epoch: 40 Loss D.: 0.6820597648620605\n",
            "Epoch: 40 Loss G.: 0.7468233704566956\n",
            "Epoch: 40 Loss D.: 0.6837354898452759\n",
            "Epoch: 40 Loss G.: 0.7480340600013733\n",
            "Epoch: 40 Loss D.: 0.6923616528511047\n",
            "Epoch: 40 Loss G.: 0.7248175144195557\n",
            "Epoch: 40 Loss D.: 0.6826786994934082\n",
            "Epoch: 40 Loss G.: 0.7480494976043701\n",
            "Epoch: 40 Loss D.: 0.6853567361831665\n",
            "Epoch: 40 Loss G.: 0.7448360323905945\n",
            "Epoch: 40 Loss D.: 0.6909937262535095\n",
            "Epoch: 40 Loss G.: 0.7259476780891418\n",
            "Epoch: 40 Loss D.: 0.6877328157424927\n",
            "Epoch: 40 Loss G.: 0.7471021413803101\n",
            "Epoch: 40 Loss D.: 0.6767029762268066\n",
            "Epoch: 40 Loss G.: 0.7137511968612671\n",
            "Epoch: 40 Loss D.: 0.6870175004005432\n",
            "Epoch: 40 Loss G.: 0.7401977181434631\n",
            "Epoch: 40 Loss D.: 0.6912367939949036\n",
            "Epoch: 40 Loss G.: 0.7165095806121826\n",
            "Epoch: 40 Loss D.: 0.6796849370002747\n",
            "Epoch: 40 Loss G.: 0.7237528562545776\n",
            "Epoch: 40 Loss D.: 0.6860148310661316\n",
            "Epoch: 40 Loss G.: 0.7452560067176819\n",
            "Epoch: 40 Loss D.: 0.6829497218132019\n",
            "Epoch: 40 Loss G.: 0.7429342269897461\n",
            "Epoch: 40 Loss D.: 0.6817929744720459\n",
            "Epoch: 40 Loss G.: 0.736984133720398\n",
            "Epoch: 40 Loss D.: 0.6799254417419434\n",
            "Epoch: 40 Loss G.: 0.733680009841919\n",
            "Epoch: 40 Loss D.: 0.6832486391067505\n",
            "Epoch: 40 Loss G.: 0.7383216023445129\n",
            "Epoch: 40 Loss D.: 0.6803048253059387\n",
            "Epoch: 40 Loss G.: 0.73514723777771\n",
            "Epoch: 40 Loss D.: 0.7003002166748047\n",
            "Epoch: 40 Loss G.: 0.7426007986068726\n",
            "Epoch: 40 Loss D.: 0.7026658058166504\n",
            "Epoch: 40 Loss G.: 0.7445924282073975\n",
            "Epoch: 40 Loss D.: 0.6936848163604736\n",
            "Epoch: 40 Loss G.: 0.7165217399597168\n",
            "Epoch: 40 Loss D.: 0.6846234202384949\n",
            "Epoch: 40 Loss G.: 0.7275605797767639\n",
            "Epoch: 40 Loss D.: 0.6990125179290771\n",
            "Epoch: 40 Loss G.: 0.697918176651001\n",
            "Epoch: 40 Loss D.: 0.6885089874267578\n",
            "Epoch: 40 Loss G.: 0.7275193333625793\n",
            "Epoch: 40 Loss D.: 0.6810898184776306\n",
            "Epoch: 40 Loss G.: 0.691985011100769\n",
            "Epoch: 40 Loss D.: 0.6985375881195068\n",
            "Epoch: 40 Loss G.: 0.723010241985321\n",
            "Epoch: 40 Loss D.: 0.701505184173584\n",
            "Epoch: 40 Loss G.: 0.7357431650161743\n",
            "Epoch: 40 Loss D.: 0.6752352714538574\n",
            "Epoch: 40 Loss G.: 0.7061761021614075\n",
            "Epoch: 40 Loss D.: 0.68497633934021\n",
            "Epoch: 40 Loss G.: 0.6960414052009583\n",
            "Epoch: 40 Loss D.: 0.6864143013954163\n",
            "Epoch: 40 Loss G.: 0.7186462879180908\n",
            "Epoch: 40 Loss D.: 0.6799838542938232\n",
            "Epoch: 40 Loss G.: 0.7126538753509521\n",
            "Epoch: 40 Loss D.: 0.6869310140609741\n",
            "Epoch: 40 Loss G.: 0.6950860619544983\n",
            "Epoch: 40 Loss D.: 0.698527991771698\n",
            "Epoch: 40 Loss G.: 0.7208046913146973\n",
            "Epoch: 50 Loss D.: 0.684608519077301\n",
            "Epoch: 50 Loss G.: 0.700730562210083\n",
            "Epoch: 50 Loss D.: 0.6804143786430359\n",
            "Epoch: 50 Loss G.: 0.7379395365715027\n",
            "Epoch: 50 Loss D.: 0.6821928024291992\n",
            "Epoch: 50 Loss G.: 0.7370743751525879\n",
            "Epoch: 50 Loss D.: 0.6800061464309692\n",
            "Epoch: 50 Loss G.: 0.7496368885040283\n",
            "Epoch: 50 Loss D.: 0.7001646757125854\n",
            "Epoch: 50 Loss G.: 0.7192649841308594\n",
            "Epoch: 50 Loss D.: 0.6864331960678101\n",
            "Epoch: 50 Loss G.: 0.7154419422149658\n",
            "Epoch: 50 Loss D.: 0.6736016869544983\n",
            "Epoch: 50 Loss G.: 0.7251803278923035\n",
            "Epoch: 50 Loss D.: 0.6858106255531311\n",
            "Epoch: 50 Loss G.: 0.7378453016281128\n",
            "Epoch: 50 Loss D.: 0.6890679597854614\n",
            "Epoch: 50 Loss G.: 0.7280725836753845\n",
            "Epoch: 50 Loss D.: 0.682653546333313\n",
            "Epoch: 50 Loss G.: 0.7282374501228333\n",
            "Epoch: 50 Loss D.: 0.6955665349960327\n",
            "Epoch: 50 Loss G.: 0.6931248903274536\n",
            "Epoch: 50 Loss D.: 0.6974788308143616\n",
            "Epoch: 50 Loss G.: 0.694628894329071\n",
            "Epoch: 50 Loss D.: 0.702174961566925\n",
            "Epoch: 50 Loss G.: 0.7100465297698975\n",
            "Epoch: 50 Loss D.: 0.6920692324638367\n",
            "Epoch: 50 Loss G.: 0.7029508352279663\n",
            "Epoch: 50 Loss D.: 0.7026435732841492\n",
            "Epoch: 50 Loss G.: 0.7309747338294983\n",
            "Epoch: 50 Loss D.: 0.6763072609901428\n",
            "Epoch: 50 Loss G.: 0.7349339723587036\n",
            "Epoch: 50 Loss D.: 0.6791766285896301\n",
            "Epoch: 50 Loss G.: 0.7325451374053955\n",
            "Epoch: 50 Loss D.: 0.6961159706115723\n",
            "Epoch: 50 Loss G.: 0.7365735173225403\n",
            "Epoch: 50 Loss D.: 0.6885291337966919\n",
            "Epoch: 50 Loss G.: 0.7561637163162231\n",
            "Epoch: 50 Loss D.: 0.6739448308944702\n",
            "Epoch: 50 Loss G.: 0.7207508683204651\n",
            "Epoch: 50 Loss D.: 0.6709482073783875\n",
            "Epoch: 50 Loss G.: 0.7162184715270996\n",
            "Epoch: 50 Loss D.: 0.6879635453224182\n",
            "Epoch: 50 Loss G.: 0.7064430117607117\n",
            "Epoch: 50 Loss D.: 0.693842351436615\n",
            "Epoch: 50 Loss G.: 0.7256460189819336\n",
            "Epoch: 50 Loss D.: 0.6887665390968323\n",
            "Epoch: 50 Loss G.: 0.7178261280059814\n",
            "Epoch: 50 Loss D.: 0.6971098184585571\n",
            "Epoch: 50 Loss G.: 0.7024589776992798\n",
            "Epoch: 50 Loss D.: 0.6942386627197266\n",
            "Epoch: 50 Loss G.: 0.7189933061599731\n",
            "Epoch: 50 Loss D.: 0.6718984246253967\n",
            "Epoch: 50 Loss G.: 0.7385275959968567\n",
            "Epoch: 50 Loss D.: 0.7017933130264282\n",
            "Epoch: 50 Loss G.: 0.7580978870391846\n",
            "Epoch: 50 Loss D.: 0.6817519664764404\n",
            "Epoch: 50 Loss G.: 0.7271538376808167\n",
            "Epoch: 50 Loss D.: 0.6893131732940674\n",
            "Epoch: 50 Loss G.: 0.7122128009796143\n",
            "Epoch: 50 Loss D.: 0.69791579246521\n",
            "Epoch: 50 Loss G.: 0.7149410843849182\n",
            "Epoch: 50 Loss D.: 0.6832257509231567\n",
            "Epoch: 50 Loss G.: 0.7325302958488464\n",
            "Epoch: 60 Loss D.: 0.6808427572250366\n",
            "Epoch: 60 Loss G.: 0.7051749229431152\n",
            "Epoch: 60 Loss D.: 0.6775243878364563\n",
            "Epoch: 60 Loss G.: 0.7032119631767273\n",
            "Epoch: 60 Loss D.: 0.6757116317749023\n",
            "Epoch: 60 Loss G.: 0.7291525602340698\n",
            "Epoch: 60 Loss D.: 0.6938793659210205\n",
            "Epoch: 60 Loss G.: 0.7314487099647522\n",
            "Epoch: 60 Loss D.: 0.6892807483673096\n",
            "Epoch: 60 Loss G.: 0.7196066379547119\n",
            "Epoch: 60 Loss D.: 0.6737125515937805\n",
            "Epoch: 60 Loss G.: 0.7134143114089966\n",
            "Epoch: 60 Loss D.: 0.6935442686080933\n",
            "Epoch: 60 Loss G.: 0.7364897727966309\n",
            "Epoch: 60 Loss D.: 0.6855384707450867\n",
            "Epoch: 60 Loss G.: 0.7013958692550659\n",
            "Epoch: 60 Loss D.: 0.690896213054657\n",
            "Epoch: 60 Loss G.: 0.739827036857605\n",
            "Epoch: 60 Loss D.: 0.6814658045768738\n",
            "Epoch: 60 Loss G.: 0.712520182132721\n",
            "Epoch: 60 Loss D.: 0.6839749813079834\n",
            "Epoch: 60 Loss G.: 0.7151886820793152\n",
            "Epoch: 60 Loss D.: 0.704887866973877\n",
            "Epoch: 60 Loss G.: 0.7523680329322815\n",
            "Epoch: 60 Loss D.: 0.6791957020759583\n",
            "Epoch: 60 Loss G.: 0.7094571590423584\n",
            "Epoch: 60 Loss D.: 0.6993153095245361\n",
            "Epoch: 60 Loss G.: 0.703888475894928\n",
            "Epoch: 60 Loss D.: 0.6724398136138916\n",
            "Epoch: 60 Loss G.: 0.7104015350341797\n",
            "Epoch: 60 Loss D.: 0.6829820871353149\n",
            "Epoch: 60 Loss G.: 0.7320270538330078\n",
            "Epoch: 60 Loss D.: 0.7157100439071655\n",
            "Epoch: 60 Loss G.: 0.7058572769165039\n",
            "Epoch: 60 Loss D.: 0.6828875541687012\n",
            "Epoch: 60 Loss G.: 0.70676589012146\n",
            "Epoch: 60 Loss D.: 0.6831430196762085\n",
            "Epoch: 60 Loss G.: 0.7463976740837097\n",
            "Epoch: 60 Loss D.: 0.6875982284545898\n",
            "Epoch: 60 Loss G.: 0.7120081186294556\n",
            "Epoch: 60 Loss D.: 0.6850908398628235\n",
            "Epoch: 60 Loss G.: 0.6981343030929565\n",
            "Epoch: 60 Loss D.: 0.6713603734970093\n",
            "Epoch: 60 Loss G.: 0.7181528806686401\n",
            "Epoch: 60 Loss D.: 0.6991962790489197\n",
            "Epoch: 60 Loss G.: 0.7530989646911621\n",
            "Epoch: 60 Loss D.: 0.6889036297798157\n",
            "Epoch: 60 Loss G.: 0.7254028916358948\n",
            "Epoch: 60 Loss D.: 0.6900997161865234\n",
            "Epoch: 60 Loss G.: 0.707364022731781\n",
            "Epoch: 60 Loss D.: 0.6999543905258179\n",
            "Epoch: 60 Loss G.: 0.7482166290283203\n",
            "Epoch: 60 Loss D.: 0.6846232414245605\n",
            "Epoch: 60 Loss G.: 0.7074893116950989\n",
            "Epoch: 60 Loss D.: 0.6791801452636719\n",
            "Epoch: 60 Loss G.: 0.6988697052001953\n",
            "Epoch: 60 Loss D.: 0.6673163771629333\n",
            "Epoch: 60 Loss G.: 0.707581639289856\n",
            "Epoch: 60 Loss D.: 0.6825742721557617\n",
            "Epoch: 60 Loss G.: 0.7010154724121094\n",
            "Epoch: 60 Loss D.: 0.6843044757843018\n",
            "Epoch: 60 Loss G.: 0.7094193696975708\n",
            "Epoch: 60 Loss D.: 0.6872758865356445\n",
            "Epoch: 60 Loss G.: 0.7226077914237976\n",
            "Epoch: 70 Loss D.: 0.6751207709312439\n",
            "Epoch: 70 Loss G.: 0.6793487071990967\n",
            "Epoch: 70 Loss D.: 0.6661121845245361\n",
            "Epoch: 70 Loss G.: 0.7348111867904663\n",
            "Epoch: 70 Loss D.: 0.6861081123352051\n",
            "Epoch: 70 Loss G.: 0.6980851292610168\n",
            "Epoch: 70 Loss D.: 0.67533278465271\n",
            "Epoch: 70 Loss G.: 0.7243964672088623\n",
            "Epoch: 70 Loss D.: 0.6863476037979126\n",
            "Epoch: 70 Loss G.: 0.702978789806366\n",
            "Epoch: 70 Loss D.: 0.7058860659599304\n",
            "Epoch: 70 Loss G.: 0.7241061329841614\n",
            "Epoch: 70 Loss D.: 0.6880776882171631\n",
            "Epoch: 70 Loss G.: 0.6938021183013916\n",
            "Epoch: 70 Loss D.: 0.6709128022193909\n",
            "Epoch: 70 Loss G.: 0.6773280501365662\n",
            "Epoch: 70 Loss D.: 0.7147328853607178\n",
            "Epoch: 70 Loss G.: 0.7096850275993347\n",
            "Epoch: 70 Loss D.: 0.6876165866851807\n",
            "Epoch: 70 Loss G.: 0.7099853754043579\n",
            "Epoch: 70 Loss D.: 0.6646168828010559\n",
            "Epoch: 70 Loss G.: 0.7394315600395203\n",
            "Epoch: 70 Loss D.: 0.6912903785705566\n",
            "Epoch: 70 Loss G.: 0.6898627281188965\n",
            "Epoch: 70 Loss D.: 0.6786479353904724\n",
            "Epoch: 70 Loss G.: 0.7498204708099365\n",
            "Epoch: 70 Loss D.: 0.7032710909843445\n",
            "Epoch: 70 Loss G.: 0.6905531883239746\n",
            "Epoch: 70 Loss D.: 0.696300745010376\n",
            "Epoch: 70 Loss G.: 0.6972113251686096\n",
            "Epoch: 70 Loss D.: 0.6879780292510986\n",
            "Epoch: 70 Loss G.: 0.695146381855011\n",
            "Epoch: 70 Loss D.: 0.6910431385040283\n",
            "Epoch: 70 Loss G.: 0.7356445789337158\n",
            "Epoch: 70 Loss D.: 0.6813749670982361\n",
            "Epoch: 70 Loss G.: 0.6989635229110718\n",
            "Epoch: 70 Loss D.: 0.6852796077728271\n",
            "Epoch: 70 Loss G.: 0.7047438621520996\n",
            "Epoch: 70 Loss D.: 0.683972954750061\n",
            "Epoch: 70 Loss G.: 0.6628880500793457\n",
            "Epoch: 70 Loss D.: 0.6951271295547485\n",
            "Epoch: 70 Loss G.: 0.6908187866210938\n",
            "Epoch: 70 Loss D.: 0.7040863633155823\n",
            "Epoch: 70 Loss G.: 0.7061567902565002\n",
            "Epoch: 70 Loss D.: 0.6909758448600769\n",
            "Epoch: 70 Loss G.: 0.6845808625221252\n",
            "Epoch: 70 Loss D.: 0.6785787343978882\n",
            "Epoch: 70 Loss G.: 0.7054486870765686\n",
            "Epoch: 70 Loss D.: 0.7091758251190186\n",
            "Epoch: 70 Loss G.: 0.6887209415435791\n",
            "Epoch: 70 Loss D.: 0.6902700662612915\n",
            "Epoch: 70 Loss G.: 0.7036914229393005\n",
            "Epoch: 70 Loss D.: 0.6774247884750366\n",
            "Epoch: 70 Loss G.: 0.7105761766433716\n",
            "Epoch: 70 Loss D.: 0.6939547657966614\n",
            "Epoch: 70 Loss G.: 0.6999186873435974\n",
            "Epoch: 70 Loss D.: 0.6696247458457947\n",
            "Epoch: 70 Loss G.: 0.6935672760009766\n",
            "Epoch: 70 Loss D.: 0.6961252093315125\n",
            "Epoch: 70 Loss G.: 0.7160994410514832\n",
            "Epoch: 70 Loss D.: 0.6628998517990112\n",
            "Epoch: 70 Loss G.: 0.6742669343948364\n",
            "Epoch: 70 Loss D.: 0.6888453364372253\n",
            "Epoch: 70 Loss G.: 0.6877635717391968\n",
            "Epoch: 80 Loss D.: 0.6882315874099731\n",
            "Epoch: 80 Loss G.: 0.6898700594902039\n",
            "Epoch: 80 Loss D.: 0.6833614706993103\n",
            "Epoch: 80 Loss G.: 0.7092908024787903\n",
            "Epoch: 80 Loss D.: 0.6965157985687256\n",
            "Epoch: 80 Loss G.: 0.6968410015106201\n",
            "Epoch: 80 Loss D.: 0.6865714192390442\n",
            "Epoch: 80 Loss G.: 0.7421540021896362\n",
            "Epoch: 80 Loss D.: 0.6890451312065125\n",
            "Epoch: 80 Loss G.: 0.7287412285804749\n",
            "Epoch: 80 Loss D.: 0.6855818033218384\n",
            "Epoch: 80 Loss G.: 0.6923338770866394\n",
            "Epoch: 80 Loss D.: 0.70038902759552\n",
            "Epoch: 80 Loss G.: 0.705733597278595\n",
            "Epoch: 80 Loss D.: 0.6890496015548706\n",
            "Epoch: 80 Loss G.: 0.6935322284698486\n",
            "Epoch: 80 Loss D.: 0.7003433704376221\n",
            "Epoch: 80 Loss G.: 0.697980523109436\n",
            "Epoch: 80 Loss D.: 0.6937094330787659\n",
            "Epoch: 80 Loss G.: 0.7089624404907227\n",
            "Epoch: 80 Loss D.: 0.6831684112548828\n",
            "Epoch: 80 Loss G.: 0.7006303668022156\n",
            "Epoch: 80 Loss D.: 0.708725094795227\n",
            "Epoch: 80 Loss G.: 0.6976961493492126\n",
            "Epoch: 80 Loss D.: 0.6920364499092102\n",
            "Epoch: 80 Loss G.: 0.7024596929550171\n",
            "Epoch: 80 Loss D.: 0.6950937509536743\n",
            "Epoch: 80 Loss G.: 0.6975378394126892\n",
            "Epoch: 80 Loss D.: 0.695339024066925\n",
            "Epoch: 80 Loss G.: 0.7039119005203247\n",
            "Epoch: 80 Loss D.: 0.6743718385696411\n",
            "Epoch: 80 Loss G.: 0.6886627078056335\n",
            "Epoch: 80 Loss D.: 0.6843773126602173\n",
            "Epoch: 80 Loss G.: 0.7022069692611694\n",
            "Epoch: 80 Loss D.: 0.6690446138381958\n",
            "Epoch: 80 Loss G.: 0.698862612247467\n",
            "Epoch: 80 Loss D.: 0.6934105157852173\n",
            "Epoch: 80 Loss G.: 0.697116494178772\n",
            "Epoch: 80 Loss D.: 0.6884287595748901\n",
            "Epoch: 80 Loss G.: 0.7015141248703003\n",
            "Epoch: 80 Loss D.: 0.6961185336112976\n",
            "Epoch: 80 Loss G.: 0.6927387118339539\n",
            "Epoch: 80 Loss D.: 0.6855491399765015\n",
            "Epoch: 80 Loss G.: 0.7228306531906128\n",
            "Epoch: 80 Loss D.: 0.6867683529853821\n",
            "Epoch: 80 Loss G.: 0.6662077903747559\n",
            "Epoch: 80 Loss D.: 0.6993663311004639\n",
            "Epoch: 80 Loss G.: 0.679866373538971\n",
            "Epoch: 80 Loss D.: 0.6943190097808838\n",
            "Epoch: 80 Loss G.: 0.7033373713493347\n",
            "Epoch: 80 Loss D.: 0.6907288432121277\n",
            "Epoch: 80 Loss G.: 0.701650857925415\n",
            "Epoch: 80 Loss D.: 0.6875526309013367\n",
            "Epoch: 80 Loss G.: 0.7119131088256836\n",
            "Epoch: 80 Loss D.: 0.7015300393104553\n",
            "Epoch: 80 Loss G.: 0.6936309337615967\n",
            "Epoch: 80 Loss D.: 0.6959403157234192\n",
            "Epoch: 80 Loss G.: 0.7089572548866272\n",
            "Epoch: 80 Loss D.: 0.701979398727417\n",
            "Epoch: 80 Loss G.: 0.7068449258804321\n",
            "Epoch: 80 Loss D.: 0.685652494430542\n",
            "Epoch: 80 Loss G.: 0.7137939929962158\n",
            "Epoch: 80 Loss D.: 0.6872012615203857\n",
            "Epoch: 80 Loss G.: 0.6919031143188477\n",
            "Epoch: 90 Loss D.: 0.6836245656013489\n",
            "Epoch: 90 Loss G.: 0.6992180347442627\n",
            "Epoch: 90 Loss D.: 0.6988546252250671\n",
            "Epoch: 90 Loss G.: 0.7087355256080627\n",
            "Epoch: 90 Loss D.: 0.7097442746162415\n",
            "Epoch: 90 Loss G.: 0.6927199363708496\n",
            "Epoch: 90 Loss D.: 0.6894720792770386\n",
            "Epoch: 90 Loss G.: 0.6854496002197266\n",
            "Epoch: 90 Loss D.: 0.7035373449325562\n",
            "Epoch: 90 Loss G.: 0.7019053101539612\n",
            "Epoch: 90 Loss D.: 0.687775731086731\n",
            "Epoch: 90 Loss G.: 0.6724928617477417\n",
            "Epoch: 90 Loss D.: 0.6900492906570435\n",
            "Epoch: 90 Loss G.: 0.6710467338562012\n",
            "Epoch: 90 Loss D.: 0.6813993453979492\n",
            "Epoch: 90 Loss G.: 0.6933519840240479\n",
            "Epoch: 90 Loss D.: 0.6948099136352539\n",
            "Epoch: 90 Loss G.: 0.6767454147338867\n",
            "Epoch: 90 Loss D.: 0.6951746344566345\n",
            "Epoch: 90 Loss G.: 0.7094839215278625\n",
            "Epoch: 90 Loss D.: 0.6901282072067261\n",
            "Epoch: 90 Loss G.: 0.6950615048408508\n",
            "Epoch: 90 Loss D.: 0.6869205236434937\n",
            "Epoch: 90 Loss G.: 0.6854325532913208\n",
            "Epoch: 90 Loss D.: 0.702581524848938\n",
            "Epoch: 90 Loss G.: 0.683561384677887\n",
            "Epoch: 90 Loss D.: 0.7025819420814514\n",
            "Epoch: 90 Loss G.: 0.7192824482917786\n",
            "Epoch: 90 Loss D.: 0.700232982635498\n",
            "Epoch: 90 Loss G.: 0.6899554133415222\n",
            "Epoch: 90 Loss D.: 0.6960561275482178\n",
            "Epoch: 90 Loss G.: 0.700539231300354\n",
            "Epoch: 90 Loss D.: 0.681328535079956\n",
            "Epoch: 90 Loss G.: 0.7021881341934204\n",
            "Epoch: 90 Loss D.: 0.6897079944610596\n",
            "Epoch: 90 Loss G.: 0.7044619917869568\n",
            "Epoch: 90 Loss D.: 0.6864800453186035\n",
            "Epoch: 90 Loss G.: 0.6907230019569397\n",
            "Epoch: 90 Loss D.: 0.6963191032409668\n",
            "Epoch: 90 Loss G.: 0.7031338214874268\n",
            "Epoch: 90 Loss D.: 0.6831726431846619\n",
            "Epoch: 90 Loss G.: 0.7026849389076233\n",
            "Epoch: 90 Loss D.: 0.7025132775306702\n",
            "Epoch: 90 Loss G.: 0.6808693408966064\n",
            "Epoch: 90 Loss D.: 0.682399570941925\n",
            "Epoch: 90 Loss G.: 0.7105193138122559\n",
            "Epoch: 90 Loss D.: 0.6859524846076965\n",
            "Epoch: 90 Loss G.: 0.7390466928482056\n",
            "Epoch: 90 Loss D.: 0.6995193362236023\n",
            "Epoch: 90 Loss G.: 0.7225033640861511\n",
            "Epoch: 90 Loss D.: 0.6958345174789429\n",
            "Epoch: 90 Loss G.: 0.6845705509185791\n",
            "Epoch: 90 Loss D.: 0.6955046653747559\n",
            "Epoch: 90 Loss G.: 0.6850606799125671\n",
            "Epoch: 90 Loss D.: 0.6812158823013306\n",
            "Epoch: 90 Loss G.: 0.6986626386642456\n",
            "Epoch: 90 Loss D.: 0.7065106630325317\n",
            "Epoch: 90 Loss G.: 0.6957778930664062\n",
            "Epoch: 90 Loss D.: 0.6900965571403503\n",
            "Epoch: 90 Loss G.: 0.6727582216262817\n",
            "Epoch: 90 Loss D.: 0.6899425983428955\n",
            "Epoch: 90 Loss G.: 0.6849564909934998\n",
            "Epoch: 90 Loss D.: 0.6946240663528442\n",
            "Epoch: 90 Loss G.: 0.6691439747810364\n",
            "Epoch: 100 Loss D.: 0.6917192935943604\n",
            "Epoch: 100 Loss G.: 0.6975964307785034\n",
            "Epoch: 100 Loss D.: 0.6935407519340515\n",
            "Epoch: 100 Loss G.: 0.6955488920211792\n",
            "Epoch: 100 Loss D.: 0.6896862983703613\n",
            "Epoch: 100 Loss G.: 0.699687123298645\n",
            "Epoch: 100 Loss D.: 0.6898385882377625\n",
            "Epoch: 100 Loss G.: 0.7087629437446594\n",
            "Epoch: 100 Loss D.: 0.6980432271957397\n",
            "Epoch: 100 Loss G.: 0.7046061754226685\n",
            "Epoch: 100 Loss D.: 0.6952232122421265\n",
            "Epoch: 100 Loss G.: 0.7073379158973694\n",
            "Epoch: 100 Loss D.: 0.6913372874259949\n",
            "Epoch: 100 Loss G.: 0.6985915899276733\n",
            "Epoch: 100 Loss D.: 0.6942903399467468\n",
            "Epoch: 100 Loss G.: 0.6941331624984741\n",
            "Epoch: 100 Loss D.: 0.6979657411575317\n",
            "Epoch: 100 Loss G.: 0.6875151991844177\n",
            "Epoch: 100 Loss D.: 0.6832650899887085\n",
            "Epoch: 100 Loss G.: 0.7095334529876709\n",
            "Epoch: 100 Loss D.: 0.7019848227500916\n",
            "Epoch: 100 Loss G.: 0.6992498636245728\n",
            "Epoch: 100 Loss D.: 0.6867784857749939\n",
            "Epoch: 100 Loss G.: 0.6899800300598145\n",
            "Epoch: 100 Loss D.: 0.6963552832603455\n",
            "Epoch: 100 Loss G.: 0.699834942817688\n",
            "Epoch: 100 Loss D.: 0.6842776536941528\n",
            "Epoch: 100 Loss G.: 0.7117958068847656\n",
            "Epoch: 100 Loss D.: 0.6962244510650635\n",
            "Epoch: 100 Loss G.: 0.6873232126235962\n",
            "Epoch: 100 Loss D.: 0.6882594227790833\n",
            "Epoch: 100 Loss G.: 0.7062952518463135\n",
            "Epoch: 100 Loss D.: 0.7020259499549866\n",
            "Epoch: 100 Loss G.: 0.6974992156028748\n",
            "Epoch: 100 Loss D.: 0.6918211579322815\n",
            "Epoch: 100 Loss G.: 0.6967730522155762\n",
            "Epoch: 100 Loss D.: 0.7031017541885376\n",
            "Epoch: 100 Loss G.: 0.6934891939163208\n",
            "Epoch: 100 Loss D.: 0.6937243342399597\n",
            "Epoch: 100 Loss G.: 0.7023380398750305\n",
            "Epoch: 100 Loss D.: 0.6879016160964966\n",
            "Epoch: 100 Loss G.: 0.695076584815979\n",
            "Epoch: 100 Loss D.: 0.695549488067627\n",
            "Epoch: 100 Loss G.: 0.7033470869064331\n",
            "Epoch: 100 Loss D.: 0.6930770874023438\n",
            "Epoch: 100 Loss G.: 0.7122642993927002\n",
            "Epoch: 100 Loss D.: 0.6816646456718445\n",
            "Epoch: 100 Loss G.: 0.711463212966919\n",
            "Epoch: 100 Loss D.: 0.6933668255805969\n",
            "Epoch: 100 Loss G.: 0.6917397975921631\n",
            "Epoch: 100 Loss D.: 0.6970614790916443\n",
            "Epoch: 100 Loss G.: 0.6835966110229492\n",
            "Epoch: 100 Loss D.: 0.6886020302772522\n",
            "Epoch: 100 Loss G.: 0.7019094228744507\n",
            "Epoch: 100 Loss D.: 0.6940294504165649\n",
            "Epoch: 100 Loss G.: 0.6867221593856812\n",
            "Epoch: 100 Loss D.: 0.6928166151046753\n",
            "Epoch: 100 Loss G.: 0.6986541748046875\n",
            "Epoch: 100 Loss D.: 0.7011290192604065\n",
            "Epoch: 100 Loss G.: 0.6960770487785339\n",
            "Epoch: 100 Loss D.: 0.7003772258758545\n",
            "Epoch: 100 Loss G.: 0.6950178742408752\n",
            "Epoch: 100 Loss D.: 0.6984957456588745\n",
            "Epoch: 100 Loss G.: 0.6931799650192261\n",
            "Epoch: 110 Loss D.: 0.6939427852630615\n",
            "Epoch: 110 Loss G.: 0.6847001314163208\n",
            "Epoch: 110 Loss D.: 0.6934638619422913\n",
            "Epoch: 110 Loss G.: 0.7117469906806946\n",
            "Epoch: 110 Loss D.: 0.6945267915725708\n",
            "Epoch: 110 Loss G.: 0.6818743944168091\n",
            "Epoch: 110 Loss D.: 0.6904535293579102\n",
            "Epoch: 110 Loss G.: 0.6778423190116882\n",
            "Epoch: 110 Loss D.: 0.6973026990890503\n",
            "Epoch: 110 Loss G.: 0.6961440443992615\n",
            "Epoch: 110 Loss D.: 0.688227653503418\n",
            "Epoch: 110 Loss G.: 0.699235737323761\n",
            "Epoch: 110 Loss D.: 0.7010588049888611\n",
            "Epoch: 110 Loss G.: 0.6948071718215942\n",
            "Epoch: 110 Loss D.: 0.6933669447898865\n",
            "Epoch: 110 Loss G.: 0.6955235004425049\n",
            "Epoch: 110 Loss D.: 0.6932148933410645\n",
            "Epoch: 110 Loss G.: 0.700945258140564\n",
            "Epoch: 110 Loss D.: 0.6919025778770447\n",
            "Epoch: 110 Loss G.: 0.6951653957366943\n",
            "Epoch: 110 Loss D.: 0.7001779675483704\n",
            "Epoch: 110 Loss G.: 0.6887465119361877\n",
            "Epoch: 110 Loss D.: 0.6916477084159851\n",
            "Epoch: 110 Loss G.: 0.6942294836044312\n",
            "Epoch: 110 Loss D.: 0.6922765970230103\n",
            "Epoch: 110 Loss G.: 0.7153356671333313\n",
            "Epoch: 110 Loss D.: 0.6972916126251221\n",
            "Epoch: 110 Loss G.: 0.6864422559738159\n",
            "Epoch: 110 Loss D.: 0.691726565361023\n",
            "Epoch: 110 Loss G.: 0.6991784572601318\n",
            "Epoch: 110 Loss D.: 0.6891919374465942\n",
            "Epoch: 110 Loss G.: 0.6984028816223145\n",
            "Epoch: 110 Loss D.: 0.6859798431396484\n",
            "Epoch: 110 Loss G.: 0.6924701929092407\n",
            "Epoch: 110 Loss D.: 0.6903934478759766\n",
            "Epoch: 110 Loss G.: 0.710067629814148\n",
            "Epoch: 110 Loss D.: 0.6929259896278381\n",
            "Epoch: 110 Loss G.: 0.6954526901245117\n",
            "Epoch: 110 Loss D.: 0.6871817708015442\n",
            "Epoch: 110 Loss G.: 0.7000510692596436\n",
            "Epoch: 110 Loss D.: 0.6940037608146667\n",
            "Epoch: 110 Loss G.: 0.7093828320503235\n",
            "Epoch: 110 Loss D.: 0.6910852789878845\n",
            "Epoch: 110 Loss G.: 0.679480254650116\n",
            "Epoch: 110 Loss D.: 0.6977152824401855\n",
            "Epoch: 110 Loss G.: 0.6977826356887817\n",
            "Epoch: 110 Loss D.: 0.6952700018882751\n",
            "Epoch: 110 Loss G.: 0.6915174722671509\n",
            "Epoch: 110 Loss D.: 0.6902762055397034\n",
            "Epoch: 110 Loss G.: 0.6972640752792358\n",
            "Epoch: 110 Loss D.: 0.7036643028259277\n",
            "Epoch: 110 Loss G.: 0.6796634197235107\n",
            "Epoch: 110 Loss D.: 0.6836788654327393\n",
            "Epoch: 110 Loss G.: 0.6957893371582031\n",
            "Epoch: 110 Loss D.: 0.6865835189819336\n",
            "Epoch: 110 Loss G.: 0.6750646829605103\n",
            "Epoch: 110 Loss D.: 0.683844804763794\n",
            "Epoch: 110 Loss G.: 0.7000778913497925\n",
            "Epoch: 110 Loss D.: 0.7035855054855347\n",
            "Epoch: 110 Loss G.: 0.6998962163925171\n",
            "Epoch: 110 Loss D.: 0.68985515832901\n",
            "Epoch: 110 Loss G.: 0.6849808692932129\n",
            "Epoch: 110 Loss D.: 0.6876047253608704\n",
            "Epoch: 110 Loss G.: 0.6830292344093323\n",
            "Epoch: 120 Loss D.: 0.6907355785369873\n",
            "Epoch: 120 Loss G.: 0.7058166265487671\n",
            "Epoch: 120 Loss D.: 0.6939696669578552\n",
            "Epoch: 120 Loss G.: 0.6977542638778687\n",
            "Epoch: 120 Loss D.: 0.697248637676239\n",
            "Epoch: 120 Loss G.: 0.7048894166946411\n",
            "Epoch: 120 Loss D.: 0.6922187805175781\n",
            "Epoch: 120 Loss G.: 0.7057015299797058\n",
            "Epoch: 120 Loss D.: 0.693351686000824\n",
            "Epoch: 120 Loss G.: 0.7020218372344971\n",
            "Epoch: 120 Loss D.: 0.699910044670105\n",
            "Epoch: 120 Loss G.: 0.6980808973312378\n",
            "Epoch: 120 Loss D.: 0.6923319101333618\n",
            "Epoch: 120 Loss G.: 0.7059386968612671\n",
            "Epoch: 120 Loss D.: 0.6898577213287354\n",
            "Epoch: 120 Loss G.: 0.6949301362037659\n",
            "Epoch: 120 Loss D.: 0.6925903558731079\n",
            "Epoch: 120 Loss G.: 0.6957077383995056\n",
            "Epoch: 120 Loss D.: 0.6900956034660339\n",
            "Epoch: 120 Loss G.: 0.7026474475860596\n",
            "Epoch: 120 Loss D.: 0.6954337358474731\n",
            "Epoch: 120 Loss G.: 0.6886805891990662\n",
            "Epoch: 120 Loss D.: 0.7000776529312134\n",
            "Epoch: 120 Loss G.: 0.6857514381408691\n",
            "Epoch: 120 Loss D.: 0.6925038695335388\n",
            "Epoch: 120 Loss G.: 0.702151894569397\n",
            "Epoch: 120 Loss D.: 0.698097825050354\n",
            "Epoch: 120 Loss G.: 0.6970097422599792\n",
            "Epoch: 120 Loss D.: 0.6916016340255737\n",
            "Epoch: 120 Loss G.: 0.6951038241386414\n",
            "Epoch: 120 Loss D.: 0.6891431212425232\n",
            "Epoch: 120 Loss G.: 0.6981825828552246\n",
            "Epoch: 120 Loss D.: 0.6886413097381592\n",
            "Epoch: 120 Loss G.: 0.6908419728279114\n",
            "Epoch: 120 Loss D.: 0.6908042430877686\n",
            "Epoch: 120 Loss G.: 0.6820896863937378\n",
            "Epoch: 120 Loss D.: 0.6940761208534241\n",
            "Epoch: 120 Loss G.: 0.6835605502128601\n",
            "Epoch: 120 Loss D.: 0.6854802966117859\n",
            "Epoch: 120 Loss G.: 0.6850935816764832\n",
            "Epoch: 120 Loss D.: 0.699805736541748\n",
            "Epoch: 120 Loss G.: 0.704952597618103\n",
            "Epoch: 120 Loss D.: 0.6877894997596741\n",
            "Epoch: 120 Loss G.: 0.6866722702980042\n",
            "Epoch: 120 Loss D.: 0.6905654668807983\n",
            "Epoch: 120 Loss G.: 0.6907635927200317\n",
            "Epoch: 120 Loss D.: 0.698427140712738\n",
            "Epoch: 120 Loss G.: 0.6861051321029663\n",
            "Epoch: 120 Loss D.: 0.692710280418396\n",
            "Epoch: 120 Loss G.: 0.6936624646186829\n",
            "Epoch: 120 Loss D.: 0.6958519220352173\n",
            "Epoch: 120 Loss G.: 0.698133111000061\n",
            "Epoch: 120 Loss D.: 0.6831635236740112\n",
            "Epoch: 120 Loss G.: 0.6842648983001709\n",
            "Epoch: 120 Loss D.: 0.6885387897491455\n",
            "Epoch: 120 Loss G.: 0.6892959475517273\n",
            "Epoch: 120 Loss D.: 0.6946305632591248\n",
            "Epoch: 120 Loss G.: 0.6954507231712341\n",
            "Epoch: 120 Loss D.: 0.6932764053344727\n",
            "Epoch: 120 Loss G.: 0.699738085269928\n",
            "Epoch: 120 Loss D.: 0.6917283535003662\n",
            "Epoch: 120 Loss G.: 0.6922525763511658\n",
            "Epoch: 120 Loss D.: 0.683689534664154\n",
            "Epoch: 120 Loss G.: 0.6996838450431824\n",
            "Epoch: 130 Loss D.: 0.6867290735244751\n",
            "Epoch: 130 Loss G.: 0.7117307186126709\n",
            "Epoch: 130 Loss D.: 0.686301589012146\n",
            "Epoch: 130 Loss G.: 0.7163989543914795\n",
            "Epoch: 130 Loss D.: 0.6901167035102844\n",
            "Epoch: 130 Loss G.: 0.6925802230834961\n",
            "Epoch: 130 Loss D.: 0.6998304128646851\n",
            "Epoch: 130 Loss G.: 0.7127714157104492\n",
            "Epoch: 130 Loss D.: 0.6954789161682129\n",
            "Epoch: 130 Loss G.: 0.7018086314201355\n",
            "Epoch: 130 Loss D.: 0.6901071667671204\n",
            "Epoch: 130 Loss G.: 0.7004181146621704\n",
            "Epoch: 130 Loss D.: 0.6957209706306458\n",
            "Epoch: 130 Loss G.: 0.7034162878990173\n",
            "Epoch: 130 Loss D.: 0.6854537129402161\n",
            "Epoch: 130 Loss G.: 0.6935853958129883\n",
            "Epoch: 130 Loss D.: 0.6871515512466431\n",
            "Epoch: 130 Loss G.: 0.7163202166557312\n",
            "Epoch: 130 Loss D.: 0.695656418800354\n",
            "Epoch: 130 Loss G.: 0.6959242820739746\n",
            "Epoch: 130 Loss D.: 0.6856684684753418\n",
            "Epoch: 130 Loss G.: 0.7188108563423157\n",
            "Epoch: 130 Loss D.: 0.6956828236579895\n",
            "Epoch: 130 Loss G.: 0.6987174153327942\n",
            "Epoch: 130 Loss D.: 0.6924259662628174\n",
            "Epoch: 130 Loss G.: 0.7092347741127014\n",
            "Epoch: 130 Loss D.: 0.6870128512382507\n",
            "Epoch: 130 Loss G.: 0.6900669932365417\n",
            "Epoch: 130 Loss D.: 0.692931592464447\n",
            "Epoch: 130 Loss G.: 0.6933686137199402\n",
            "Epoch: 130 Loss D.: 0.7007679343223572\n",
            "Epoch: 130 Loss G.: 0.7010065317153931\n",
            "Epoch: 130 Loss D.: 0.6912673711776733\n",
            "Epoch: 130 Loss G.: 0.6991636157035828\n",
            "Epoch: 130 Loss D.: 0.6867719888687134\n",
            "Epoch: 130 Loss G.: 0.6867504119873047\n",
            "Epoch: 130 Loss D.: 0.7028795480728149\n",
            "Epoch: 130 Loss G.: 0.6955158710479736\n",
            "Epoch: 130 Loss D.: 0.6952680945396423\n",
            "Epoch: 130 Loss G.: 0.6873623132705688\n",
            "Epoch: 130 Loss D.: 0.6830954551696777\n",
            "Epoch: 130 Loss G.: 0.6850985884666443\n",
            "Epoch: 130 Loss D.: 0.6943842172622681\n",
            "Epoch: 130 Loss G.: 0.6801678538322449\n",
            "Epoch: 130 Loss D.: 0.6959241032600403\n",
            "Epoch: 130 Loss G.: 0.6954648494720459\n",
            "Epoch: 130 Loss D.: 0.6874681711196899\n",
            "Epoch: 130 Loss G.: 0.7023321390151978\n",
            "Epoch: 130 Loss D.: 0.6930717825889587\n",
            "Epoch: 130 Loss G.: 0.6914738416671753\n",
            "Epoch: 130 Loss D.: 0.6954407691955566\n",
            "Epoch: 130 Loss G.: 0.6852676272392273\n",
            "Epoch: 130 Loss D.: 0.7002003788948059\n",
            "Epoch: 130 Loss G.: 0.694413423538208\n",
            "Epoch: 130 Loss D.: 0.7003212571144104\n",
            "Epoch: 130 Loss G.: 0.6900662183761597\n",
            "Epoch: 130 Loss D.: 0.6944027543067932\n",
            "Epoch: 130 Loss G.: 0.708568274974823\n",
            "Epoch: 130 Loss D.: 0.6953105330467224\n",
            "Epoch: 130 Loss G.: 0.6877396702766418\n",
            "Epoch: 130 Loss D.: 0.700255811214447\n",
            "Epoch: 130 Loss G.: 0.6958997249603271\n",
            "Epoch: 130 Loss D.: 0.6947346925735474\n",
            "Epoch: 130 Loss G.: 0.6826479434967041\n",
            "Epoch: 140 Loss D.: 0.6945668458938599\n",
            "Epoch: 140 Loss G.: 0.6883805990219116\n",
            "Epoch: 140 Loss D.: 0.6923830509185791\n",
            "Epoch: 140 Loss G.: 0.6810864210128784\n",
            "Epoch: 140 Loss D.: 0.6972684860229492\n",
            "Epoch: 140 Loss G.: 0.7000194787979126\n",
            "Epoch: 140 Loss D.: 0.7017695903778076\n",
            "Epoch: 140 Loss G.: 0.6936504244804382\n",
            "Epoch: 140 Loss D.: 0.6860315799713135\n",
            "Epoch: 140 Loss G.: 0.6873630285263062\n",
            "Epoch: 140 Loss D.: 0.6979207992553711\n",
            "Epoch: 140 Loss G.: 0.6919127106666565\n",
            "Epoch: 140 Loss D.: 0.6912331581115723\n",
            "Epoch: 140 Loss G.: 0.6894890666007996\n",
            "Epoch: 140 Loss D.: 0.6940352916717529\n",
            "Epoch: 140 Loss G.: 0.6815980076789856\n",
            "Epoch: 140 Loss D.: 0.6977002620697021\n",
            "Epoch: 140 Loss G.: 0.6914069056510925\n",
            "Epoch: 140 Loss D.: 0.6993075609207153\n",
            "Epoch: 140 Loss G.: 0.6871904134750366\n",
            "Epoch: 140 Loss D.: 0.6955959796905518\n",
            "Epoch: 140 Loss G.: 0.6857412457466125\n",
            "Epoch: 140 Loss D.: 0.6919820308685303\n",
            "Epoch: 140 Loss G.: 0.6975393891334534\n",
            "Epoch: 140 Loss D.: 0.6941648721694946\n",
            "Epoch: 140 Loss G.: 0.6884543299674988\n",
            "Epoch: 140 Loss D.: 0.6911281943321228\n",
            "Epoch: 140 Loss G.: 0.6969757080078125\n",
            "Epoch: 140 Loss D.: 0.6981368064880371\n",
            "Epoch: 140 Loss G.: 0.6909844279289246\n",
            "Epoch: 140 Loss D.: 0.6978079080581665\n",
            "Epoch: 140 Loss G.: 0.6973837018013\n",
            "Epoch: 140 Loss D.: 0.6930171251296997\n",
            "Epoch: 140 Loss G.: 0.6855711340904236\n",
            "Epoch: 140 Loss D.: 0.6924294829368591\n",
            "Epoch: 140 Loss G.: 0.6864533424377441\n",
            "Epoch: 140 Loss D.: 0.6948105096817017\n",
            "Epoch: 140 Loss G.: 0.692219078540802\n",
            "Epoch: 140 Loss D.: 0.69207364320755\n",
            "Epoch: 140 Loss G.: 0.685524046421051\n",
            "Epoch: 140 Loss D.: 0.694446861743927\n",
            "Epoch: 140 Loss G.: 0.6810107231140137\n",
            "Epoch: 140 Loss D.: 0.6896564960479736\n",
            "Epoch: 140 Loss G.: 0.699242353439331\n",
            "Epoch: 140 Loss D.: 0.694367527961731\n",
            "Epoch: 140 Loss G.: 0.6884765625\n",
            "Epoch: 140 Loss D.: 0.6966442465782166\n",
            "Epoch: 140 Loss G.: 0.69532310962677\n",
            "Epoch: 140 Loss D.: 0.7010167241096497\n",
            "Epoch: 140 Loss G.: 0.6968284845352173\n",
            "Epoch: 140 Loss D.: 0.6926592588424683\n",
            "Epoch: 140 Loss G.: 0.6843089461326599\n",
            "Epoch: 140 Loss D.: 0.6911246180534363\n",
            "Epoch: 140 Loss G.: 0.6890383958816528\n",
            "Epoch: 140 Loss D.: 0.691236674785614\n",
            "Epoch: 140 Loss G.: 0.6921905279159546\n",
            "Epoch: 140 Loss D.: 0.6950969099998474\n",
            "Epoch: 140 Loss G.: 0.6851383447647095\n",
            "Epoch: 140 Loss D.: 0.6879277229309082\n",
            "Epoch: 140 Loss G.: 0.6919791102409363\n",
            "Epoch: 140 Loss D.: 0.6892725825309753\n",
            "Epoch: 140 Loss G.: 0.6875311732292175\n",
            "Epoch: 140 Loss D.: 0.6970973014831543\n",
            "Epoch: 140 Loss G.: 0.6842320561408997\n",
            "Epoch: 150 Loss D.: 0.690122127532959\n",
            "Epoch: 150 Loss G.: 0.6897119283676147\n",
            "Epoch: 150 Loss D.: 0.6884682178497314\n",
            "Epoch: 150 Loss G.: 0.6958361268043518\n",
            "Epoch: 150 Loss D.: 0.6993159055709839\n",
            "Epoch: 150 Loss G.: 0.69056236743927\n",
            "Epoch: 150 Loss D.: 0.6864947080612183\n",
            "Epoch: 150 Loss G.: 0.696444571018219\n",
            "Epoch: 150 Loss D.: 0.6929028034210205\n",
            "Epoch: 150 Loss G.: 0.6939742565155029\n",
            "Epoch: 150 Loss D.: 0.6864479780197144\n",
            "Epoch: 150 Loss G.: 0.693181574344635\n",
            "Epoch: 150 Loss D.: 0.6882129311561584\n",
            "Epoch: 150 Loss G.: 0.6920452117919922\n",
            "Epoch: 150 Loss D.: 0.6866395473480225\n",
            "Epoch: 150 Loss G.: 0.6914809942245483\n",
            "Epoch: 150 Loss D.: 0.691417932510376\n",
            "Epoch: 150 Loss G.: 0.7007434964179993\n",
            "Epoch: 150 Loss D.: 0.6888868808746338\n",
            "Epoch: 150 Loss G.: 0.7021494507789612\n",
            "Epoch: 150 Loss D.: 0.6925559043884277\n",
            "Epoch: 150 Loss G.: 0.6995962262153625\n",
            "Epoch: 150 Loss D.: 0.6944165229797363\n",
            "Epoch: 150 Loss G.: 0.7057939767837524\n",
            "Epoch: 150 Loss D.: 0.687603235244751\n",
            "Epoch: 150 Loss G.: 0.6861377954483032\n",
            "Epoch: 150 Loss D.: 0.6954127550125122\n",
            "Epoch: 150 Loss G.: 0.691259503364563\n",
            "Epoch: 150 Loss D.: 0.6928511261940002\n",
            "Epoch: 150 Loss G.: 0.695758581161499\n",
            "Epoch: 150 Loss D.: 0.6911534667015076\n",
            "Epoch: 150 Loss G.: 0.6873199343681335\n",
            "Epoch: 150 Loss D.: 0.6922230124473572\n",
            "Epoch: 150 Loss G.: 0.6875526309013367\n",
            "Epoch: 150 Loss D.: 0.6947692036628723\n",
            "Epoch: 150 Loss G.: 0.6951269507408142\n",
            "Epoch: 150 Loss D.: 0.6932392716407776\n",
            "Epoch: 150 Loss G.: 0.6824865341186523\n",
            "Epoch: 150 Loss D.: 0.6931942701339722\n",
            "Epoch: 150 Loss G.: 0.6956050395965576\n",
            "Epoch: 150 Loss D.: 0.6949235200881958\n",
            "Epoch: 150 Loss G.: 0.7006044983863831\n",
            "Epoch: 150 Loss D.: 0.6922364830970764\n",
            "Epoch: 150 Loss G.: 0.6874767541885376\n",
            "Epoch: 150 Loss D.: 0.6910948753356934\n",
            "Epoch: 150 Loss G.: 0.6908065676689148\n",
            "Epoch: 150 Loss D.: 0.6936482191085815\n",
            "Epoch: 150 Loss G.: 0.6845840215682983\n",
            "Epoch: 150 Loss D.: 0.695712685585022\n",
            "Epoch: 150 Loss G.: 0.6861193180084229\n",
            "Epoch: 150 Loss D.: 0.6872897148132324\n",
            "Epoch: 150 Loss G.: 0.6856073141098022\n",
            "Epoch: 150 Loss D.: 0.6927366256713867\n",
            "Epoch: 150 Loss G.: 0.6892065405845642\n",
            "Epoch: 150 Loss D.: 0.6944921612739563\n",
            "Epoch: 150 Loss G.: 0.7015199661254883\n",
            "Epoch: 150 Loss D.: 0.6826286911964417\n",
            "Epoch: 150 Loss G.: 0.6928523182868958\n",
            "Epoch: 150 Loss D.: 0.6935008764266968\n",
            "Epoch: 150 Loss G.: 0.6964566707611084\n",
            "Epoch: 150 Loss D.: 0.6888771653175354\n",
            "Epoch: 150 Loss G.: 0.6883596777915955\n",
            "Epoch: 150 Loss D.: 0.6920607686042786\n",
            "Epoch: 150 Loss G.: 0.6925723552703857\n",
            "Epoch: 160 Loss D.: 0.6960341930389404\n",
            "Epoch: 160 Loss G.: 0.6860508918762207\n",
            "Epoch: 160 Loss D.: 0.6855735182762146\n",
            "Epoch: 160 Loss G.: 0.6870411038398743\n",
            "Epoch: 160 Loss D.: 0.6960810422897339\n",
            "Epoch: 160 Loss G.: 0.6914973258972168\n",
            "Epoch: 160 Loss D.: 0.6875370740890503\n",
            "Epoch: 160 Loss G.: 0.6867844462394714\n",
            "Epoch: 160 Loss D.: 0.6911851167678833\n",
            "Epoch: 160 Loss G.: 0.6807342767715454\n",
            "Epoch: 160 Loss D.: 0.6927449703216553\n",
            "Epoch: 160 Loss G.: 0.6801953315734863\n",
            "Epoch: 160 Loss D.: 0.6899782419204712\n",
            "Epoch: 160 Loss G.: 0.6764459013938904\n",
            "Epoch: 160 Loss D.: 0.6910017728805542\n",
            "Epoch: 160 Loss G.: 0.6844595670700073\n",
            "Epoch: 160 Loss D.: 0.69025719165802\n",
            "Epoch: 160 Loss G.: 0.6731055974960327\n",
            "Epoch: 160 Loss D.: 0.6940727233886719\n",
            "Epoch: 160 Loss G.: 0.6863541603088379\n",
            "Epoch: 160 Loss D.: 0.6876852512359619\n",
            "Epoch: 160 Loss G.: 0.678214967250824\n",
            "Epoch: 160 Loss D.: 0.6946032643318176\n",
            "Epoch: 160 Loss G.: 0.6893536448478699\n",
            "Epoch: 160 Loss D.: 0.6896010637283325\n",
            "Epoch: 160 Loss G.: 0.6905510425567627\n",
            "Epoch: 160 Loss D.: 0.7003116011619568\n",
            "Epoch: 160 Loss G.: 0.6974778771400452\n",
            "Epoch: 160 Loss D.: 0.689954936504364\n",
            "Epoch: 160 Loss G.: 0.6919437646865845\n",
            "Epoch: 160 Loss D.: 0.6933495998382568\n",
            "Epoch: 160 Loss G.: 0.6861098408699036\n",
            "Epoch: 160 Loss D.: 0.6978583335876465\n",
            "Epoch: 160 Loss G.: 0.6971240043640137\n",
            "Epoch: 160 Loss D.: 0.6945441365242004\n",
            "Epoch: 160 Loss G.: 0.6901522278785706\n",
            "Epoch: 160 Loss D.: 0.6903710961341858\n",
            "Epoch: 160 Loss G.: 0.6925311088562012\n",
            "Epoch: 160 Loss D.: 0.6890571117401123\n",
            "Epoch: 160 Loss G.: 0.6938654780387878\n",
            "Epoch: 160 Loss D.: 0.692438006401062\n",
            "Epoch: 160 Loss G.: 0.6912268996238708\n",
            "Epoch: 160 Loss D.: 0.6999964714050293\n",
            "Epoch: 160 Loss G.: 0.6917475461959839\n",
            "Epoch: 160 Loss D.: 0.6975955367088318\n",
            "Epoch: 160 Loss G.: 0.6983409523963928\n",
            "Epoch: 160 Loss D.: 0.6871384382247925\n",
            "Epoch: 160 Loss G.: 0.6836749911308289\n",
            "Epoch: 160 Loss D.: 0.696591854095459\n",
            "Epoch: 160 Loss G.: 0.6845088601112366\n",
            "Epoch: 160 Loss D.: 0.6923936009407043\n",
            "Epoch: 160 Loss G.: 0.6918917298316956\n",
            "Epoch: 160 Loss D.: 0.6939833164215088\n",
            "Epoch: 160 Loss G.: 0.6880851984024048\n",
            "Epoch: 160 Loss D.: 0.6961390376091003\n",
            "Epoch: 160 Loss G.: 0.684191882610321\n",
            "Epoch: 160 Loss D.: 0.6917420625686646\n",
            "Epoch: 160 Loss G.: 0.6807777285575867\n",
            "Epoch: 160 Loss D.: 0.6964778900146484\n",
            "Epoch: 160 Loss G.: 0.694546639919281\n",
            "Epoch: 160 Loss D.: 0.6936790347099304\n",
            "Epoch: 160 Loss G.: 0.6955516338348389\n",
            "Epoch: 160 Loss D.: 0.6931378841400146\n",
            "Epoch: 160 Loss G.: 0.7074276804924011\n",
            "Epoch: 170 Loss D.: 0.6984305381774902\n",
            "Epoch: 170 Loss G.: 0.684701681137085\n",
            "Epoch: 170 Loss D.: 0.6930648684501648\n",
            "Epoch: 170 Loss G.: 0.6868614554405212\n",
            "Epoch: 170 Loss D.: 0.6934233903884888\n",
            "Epoch: 170 Loss G.: 0.6862554550170898\n",
            "Epoch: 170 Loss D.: 0.6908817887306213\n",
            "Epoch: 170 Loss G.: 0.6968948245048523\n",
            "Epoch: 170 Loss D.: 0.6906708478927612\n",
            "Epoch: 170 Loss G.: 0.6910848617553711\n",
            "Epoch: 170 Loss D.: 0.6883946657180786\n",
            "Epoch: 170 Loss G.: 0.6969007849693298\n",
            "Epoch: 170 Loss D.: 0.6903682947158813\n",
            "Epoch: 170 Loss G.: 0.6924939155578613\n",
            "Epoch: 170 Loss D.: 0.6920775771141052\n",
            "Epoch: 170 Loss G.: 0.6916247606277466\n",
            "Epoch: 170 Loss D.: 0.6925053000450134\n",
            "Epoch: 170 Loss G.: 0.6835918426513672\n",
            "Epoch: 170 Loss D.: 0.6928445100784302\n",
            "Epoch: 170 Loss G.: 0.6892022490501404\n",
            "Epoch: 170 Loss D.: 0.6945523023605347\n",
            "Epoch: 170 Loss G.: 0.6891742944717407\n",
            "Epoch: 170 Loss D.: 0.6954717040061951\n",
            "Epoch: 170 Loss G.: 0.6886645555496216\n",
            "Epoch: 170 Loss D.: 0.6907961368560791\n",
            "Epoch: 170 Loss G.: 0.6925761699676514\n",
            "Epoch: 170 Loss D.: 0.6926178336143494\n",
            "Epoch: 170 Loss G.: 0.691674530506134\n",
            "Epoch: 170 Loss D.: 0.689161479473114\n",
            "Epoch: 170 Loss G.: 0.6847588419914246\n",
            "Epoch: 170 Loss D.: 0.6950504183769226\n",
            "Epoch: 170 Loss G.: 0.6933982372283936\n",
            "Epoch: 170 Loss D.: 0.6962565779685974\n",
            "Epoch: 170 Loss G.: 0.6894591450691223\n",
            "Epoch: 170 Loss D.: 0.6968861818313599\n",
            "Epoch: 170 Loss G.: 0.6878076195716858\n",
            "Epoch: 170 Loss D.: 0.690792441368103\n",
            "Epoch: 170 Loss G.: 0.6890771389007568\n",
            "Epoch: 170 Loss D.: 0.6957591772079468\n",
            "Epoch: 170 Loss G.: 0.6830596923828125\n",
            "Epoch: 170 Loss D.: 0.6988422870635986\n",
            "Epoch: 170 Loss G.: 0.6881150007247925\n",
            "Epoch: 170 Loss D.: 0.6925591230392456\n",
            "Epoch: 170 Loss G.: 0.7022548913955688\n",
            "Epoch: 170 Loss D.: 0.6979594230651855\n",
            "Epoch: 170 Loss G.: 0.6954531669616699\n",
            "Epoch: 170 Loss D.: 0.6941558122634888\n",
            "Epoch: 170 Loss G.: 0.6862074136734009\n",
            "Epoch: 170 Loss D.: 0.6876693964004517\n",
            "Epoch: 170 Loss G.: 0.698043704032898\n",
            "Epoch: 170 Loss D.: 0.6939246654510498\n",
            "Epoch: 170 Loss G.: 0.697523295879364\n",
            "Epoch: 170 Loss D.: 0.6996652483940125\n",
            "Epoch: 170 Loss G.: 0.6930614113807678\n",
            "Epoch: 170 Loss D.: 0.6977721452713013\n",
            "Epoch: 170 Loss G.: 0.6935110092163086\n",
            "Epoch: 170 Loss D.: 0.6878467798233032\n",
            "Epoch: 170 Loss G.: 0.6830664873123169\n",
            "Epoch: 170 Loss D.: 0.6967926025390625\n",
            "Epoch: 170 Loss G.: 0.6880781054496765\n",
            "Epoch: 170 Loss D.: 0.6930537223815918\n",
            "Epoch: 170 Loss G.: 0.6965398192405701\n",
            "Epoch: 170 Loss D.: 0.6889265179634094\n",
            "Epoch: 170 Loss G.: 0.6831991076469421\n",
            "Epoch: 180 Loss D.: 0.6963369846343994\n",
            "Epoch: 180 Loss G.: 0.6879766583442688\n",
            "Epoch: 180 Loss D.: 0.6899352669715881\n",
            "Epoch: 180 Loss G.: 0.6946213841438293\n",
            "Epoch: 180 Loss D.: 0.6919085383415222\n",
            "Epoch: 180 Loss G.: 0.6822503805160522\n",
            "Epoch: 180 Loss D.: 0.6941191554069519\n",
            "Epoch: 180 Loss G.: 0.6975403428077698\n",
            "Epoch: 180 Loss D.: 0.6962896585464478\n",
            "Epoch: 180 Loss G.: 0.6906715035438538\n",
            "Epoch: 180 Loss D.: 0.6999689340591431\n",
            "Epoch: 180 Loss G.: 0.7018605470657349\n",
            "Epoch: 180 Loss D.: 0.6903302073478699\n",
            "Epoch: 180 Loss G.: 0.6976370811462402\n",
            "Epoch: 180 Loss D.: 0.6945362091064453\n",
            "Epoch: 180 Loss G.: 0.6868395805358887\n",
            "Epoch: 180 Loss D.: 0.6953997611999512\n",
            "Epoch: 180 Loss G.: 0.6870063543319702\n",
            "Epoch: 180 Loss D.: 0.6901659965515137\n",
            "Epoch: 180 Loss G.: 0.6824202537536621\n",
            "Epoch: 180 Loss D.: 0.6934135556221008\n",
            "Epoch: 180 Loss G.: 0.6902681589126587\n",
            "Epoch: 180 Loss D.: 0.6916797161102295\n",
            "Epoch: 180 Loss G.: 0.6928290724754333\n",
            "Epoch: 180 Loss D.: 0.6879282593727112\n",
            "Epoch: 180 Loss G.: 0.7011415362358093\n",
            "Epoch: 180 Loss D.: 0.6976364850997925\n",
            "Epoch: 180 Loss G.: 0.6931604743003845\n",
            "Epoch: 180 Loss D.: 0.6864104270935059\n",
            "Epoch: 180 Loss G.: 0.6894605159759521\n",
            "Epoch: 180 Loss D.: 0.6927837133407593\n",
            "Epoch: 180 Loss G.: 0.6938380599021912\n",
            "Epoch: 180 Loss D.: 0.6942181587219238\n",
            "Epoch: 180 Loss G.: 0.6909089088439941\n",
            "Epoch: 180 Loss D.: 0.6927525997161865\n",
            "Epoch: 180 Loss G.: 0.6902482509613037\n",
            "Epoch: 180 Loss D.: 0.689624547958374\n",
            "Epoch: 180 Loss G.: 0.6892662644386292\n",
            "Epoch: 180 Loss D.: 0.6942844986915588\n",
            "Epoch: 180 Loss G.: 0.6912665963172913\n",
            "Epoch: 180 Loss D.: 0.6948602795600891\n",
            "Epoch: 180 Loss G.: 0.6893774271011353\n",
            "Epoch: 180 Loss D.: 0.6948944330215454\n",
            "Epoch: 180 Loss G.: 0.6949792504310608\n",
            "Epoch: 180 Loss D.: 0.692673921585083\n",
            "Epoch: 180 Loss G.: 0.6882486343383789\n",
            "Epoch: 180 Loss D.: 0.6865103244781494\n",
            "Epoch: 180 Loss G.: 0.700799822807312\n",
            "Epoch: 180 Loss D.: 0.6946150064468384\n",
            "Epoch: 180 Loss G.: 0.6910725235939026\n",
            "Epoch: 180 Loss D.: 0.6926196813583374\n",
            "Epoch: 180 Loss G.: 0.6897757053375244\n",
            "Epoch: 180 Loss D.: 0.6947172284126282\n",
            "Epoch: 180 Loss G.: 0.6833705902099609\n",
            "Epoch: 180 Loss D.: 0.6964131593704224\n",
            "Epoch: 180 Loss G.: 0.6941820979118347\n",
            "Epoch: 180 Loss D.: 0.693554162979126\n",
            "Epoch: 180 Loss G.: 0.6926838159561157\n",
            "Epoch: 180 Loss D.: 0.693313717842102\n",
            "Epoch: 180 Loss G.: 0.704809844493866\n",
            "Epoch: 180 Loss D.: 0.6939060688018799\n",
            "Epoch: 180 Loss G.: 0.6847589015960693\n",
            "Epoch: 180 Loss D.: 0.6986142992973328\n",
            "Epoch: 180 Loss G.: 0.6887539625167847\n",
            "Epoch: 190 Loss D.: 0.6949141025543213\n",
            "Epoch: 190 Loss G.: 0.697182834148407\n",
            "Epoch: 190 Loss D.: 0.6863663196563721\n",
            "Epoch: 190 Loss G.: 0.6993717551231384\n",
            "Epoch: 190 Loss D.: 0.6945435404777527\n",
            "Epoch: 190 Loss G.: 0.6931608319282532\n",
            "Epoch: 190 Loss D.: 0.6959083080291748\n",
            "Epoch: 190 Loss G.: 0.7092345356941223\n",
            "Epoch: 190 Loss D.: 0.6862753033638\n",
            "Epoch: 190 Loss G.: 0.7091948390007019\n",
            "Epoch: 190 Loss D.: 0.6874534487724304\n",
            "Epoch: 190 Loss G.: 0.7015442848205566\n",
            "Epoch: 190 Loss D.: 0.7010217308998108\n",
            "Epoch: 190 Loss G.: 0.7205368876457214\n",
            "Epoch: 190 Loss D.: 0.6922958493232727\n",
            "Epoch: 190 Loss G.: 0.6967681646347046\n",
            "Epoch: 190 Loss D.: 0.6933467388153076\n",
            "Epoch: 190 Loss G.: 0.6898927688598633\n",
            "Epoch: 190 Loss D.: 0.6968730092048645\n",
            "Epoch: 190 Loss G.: 0.703929603099823\n",
            "Epoch: 190 Loss D.: 0.6888164281845093\n",
            "Epoch: 190 Loss G.: 0.700721025466919\n",
            "Epoch: 190 Loss D.: 0.6942753195762634\n",
            "Epoch: 190 Loss G.: 0.6990317106246948\n",
            "Epoch: 190 Loss D.: 0.694001317024231\n",
            "Epoch: 190 Loss G.: 0.6938344240188599\n",
            "Epoch: 190 Loss D.: 0.6939897537231445\n",
            "Epoch: 190 Loss G.: 0.6832571625709534\n",
            "Epoch: 190 Loss D.: 0.6971173882484436\n",
            "Epoch: 190 Loss G.: 0.7048128843307495\n",
            "Epoch: 190 Loss D.: 0.6965660452842712\n",
            "Epoch: 190 Loss G.: 0.6897069215774536\n",
            "Epoch: 190 Loss D.: 0.6999807357788086\n",
            "Epoch: 190 Loss G.: 0.6943387389183044\n",
            "Epoch: 190 Loss D.: 0.697425901889801\n",
            "Epoch: 190 Loss G.: 0.6828464269638062\n",
            "Epoch: 190 Loss D.: 0.6994059681892395\n",
            "Epoch: 190 Loss G.: 0.693121075630188\n",
            "Epoch: 190 Loss D.: 0.695575475692749\n",
            "Epoch: 190 Loss G.: 0.6972433924674988\n",
            "Epoch: 190 Loss D.: 0.6849538087844849\n",
            "Epoch: 190 Loss G.: 0.6858468055725098\n",
            "Epoch: 190 Loss D.: 0.6932587623596191\n",
            "Epoch: 190 Loss G.: 0.6884223818778992\n",
            "Epoch: 190 Loss D.: 0.6912201642990112\n",
            "Epoch: 190 Loss G.: 0.6916852593421936\n",
            "Epoch: 190 Loss D.: 0.6911913156509399\n",
            "Epoch: 190 Loss G.: 0.6996223330497742\n",
            "Epoch: 190 Loss D.: 0.6941098570823669\n",
            "Epoch: 190 Loss G.: 0.6899442672729492\n",
            "Epoch: 190 Loss D.: 0.6945432424545288\n",
            "Epoch: 190 Loss G.: 0.7038903832435608\n",
            "Epoch: 190 Loss D.: 0.6942180395126343\n",
            "Epoch: 190 Loss G.: 0.6966266632080078\n",
            "Epoch: 190 Loss D.: 0.6863265037536621\n",
            "Epoch: 190 Loss G.: 0.6951465010643005\n",
            "Epoch: 190 Loss D.: 0.6953599452972412\n",
            "Epoch: 190 Loss G.: 0.7014210820198059\n",
            "Epoch: 190 Loss D.: 0.6968962550163269\n",
            "Epoch: 190 Loss G.: 0.7005633115768433\n",
            "Epoch: 190 Loss D.: 0.6938991546630859\n",
            "Epoch: 190 Loss G.: 0.6872493028640747\n",
            "Epoch: 190 Loss D.: 0.6925331354141235\n",
            "Epoch: 190 Loss G.: 0.6929408311843872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space_samples = torch.randn(100, 2)\n",
        "generated_samples = generator(latent_space_samples)"
      ],
      "metadata": {
        "id": "MV3D3ZI_3p_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_samples = generated_samples.detach()\n",
        "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n",
        "plt.plot(train_data[:, 0], train_data[:, 1], \".\")"
      ],
      "metadata": {
        "id": "eXpnh_4F3qzc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8ae99cb8-0b27-4176-dc93-0dc0d2556092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd14beba850>]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}